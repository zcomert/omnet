{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QaFrvOWeGWoE",
        "outputId": "1ccbdb00-53d4-4cb3-b363-835012f149de",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ls \"/content/drive/My Drive/kulak_colab/augmented_4/data/train/\"\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from PIL import Image\n",
        "from albumentations import *\n",
        "from skimage.transform import resize\n",
        "\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "from keras.layers import *\n",
        "from keras.callbacks import *\n",
        "from keras.optimizers import *\n",
        "from keras.models import load_model, Model\n",
        "import tensorflow\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.callbacks import TensorBoard"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "aom  csom  earwax  normal\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oY4ffYPhGdIE",
        "colab": {}
      },
      "source": [
        "SHAPE = (224, 224, 3)\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 100\n",
        "N_SPLITS = 5\n",
        "SEED = 1881\n",
        "TRAIN_TEST_RATIO = 0.2\n",
        "\n",
        "TOTAL_CLASS_NUMBER = 4 # 4, 5\n",
        "AUGMENT_BOOL = True\n",
        "\n",
        "BASE_DIR_TRAIN = \"/content/drive/My Drive/kulak_colab/augmented_4/data/train/\"\n",
        "BASE_DIR_TEST = \"/content/drive/My Drive/kulak_colab/augmented_4/data/test/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uGT_8VyTZt2T",
        "colab": {}
      },
      "source": [
        "class DATASET:\n",
        "    \n",
        "    \"\"\"\n",
        "    input_shape           --> TUPLE.wanted image size\n",
        "    batch_size            --> INT.yielding data size for every iteration\n",
        "    orders                --> LIST.which images will be used. max=len(all_images). it can be used for K-fold(CV).\n",
        "    base_dir_train        --> STR.the DIR which is include training images.\n",
        "    base_dir_test         --> STR.the DIR which is include test images.\n",
        "    seed                  --> INT. This allow to dataset generator to more reproduciable and it ensures that x and y are shuffled with compatible.\n",
        "    augment               --> BOOL. Augment data or not.\n",
        "    train_test_ratio      --> FLOAT.How much of data will be used as test set.\n",
        "    class_number          --> INT.Prepare label for 4 or 5 classes.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_shape, batch_size, orders, base_dir_train, base_dir_test, seed, train_test_ratio, class_number, augment=True):\n",
        "        self.SHAPE                 = input_shape\n",
        "        self.BATCH_SIZE            = batch_size\n",
        "        self.arr                   = orders\n",
        "        self.SEED                  = seed\n",
        "        self.TT_RATIO              = train_test_ratio\n",
        "        self.AUG                   = augment\n",
        "        \n",
        "        self.CLASS_NUMBER = class_number\n",
        "        self.BASE_DIR_TRAIN = base_dir_train\n",
        "        self.BASE_DIR_TEST = base_dir_test\n",
        "        \n",
        "        \n",
        "    def get_train_paths_n_labels(self):\n",
        "\n",
        "        x      = []\n",
        "        label = []\n",
        "\n",
        "        img_paths = glob.glob(BASE_DIR_TRAIN+\"*/*.png\")\n",
        "        \n",
        "        for img_path in img_paths:\n",
        "            \n",
        "            if self.CLASS_NUMBER==5:\n",
        "                if \"normal\" in img_path:\n",
        "                    x.append(img_path)\n",
        "                    label.append([1,0,0,0,0])\n",
        "                elif \"aom\" in img_path:\n",
        "                    x.append(img_path)\n",
        "                    label.append([0,1,0,0,0])\n",
        "                elif \"csom\" in img_path:\n",
        "                    x.append(img_path)\n",
        "                    label.append([0,0,1,0,0])\n",
        "                elif \"earwax\" in img_path:\n",
        "                    x.append(img_path)\n",
        "                    label.append([0,0,0,1,0])\n",
        "\n",
        "                elif \"other\" in img_path:\n",
        "                        x.append(img_path)\n",
        "                        label.append([0,0,0,0,1])\n",
        "                    \n",
        "            elif self.CLASS_NUMBER==4:\n",
        "                if \"normal\" in img_path:\n",
        "                    x.append(img_path)\n",
        "                    label.append([1,0,0,0])\n",
        "                elif \"aom\" in img_path:\n",
        "                    x.append(img_path)\n",
        "                    label.append([0,1,0,0])\n",
        "                elif \"csom\" in img_path:\n",
        "                    x.append(img_path)\n",
        "                    label.append([0,0,1,0])\n",
        "                elif \"earwax\" in img_path:\n",
        "                    x.append(img_path)\n",
        "                    label.append([0,0,0,1])\n",
        "                \n",
        "        return x, label\n",
        "    \n",
        "    def get_test_paths_n_labels(self):\n",
        "\n",
        "        x      = []\n",
        "        label = []\n",
        "\n",
        "        img_paths = glob.glob(BASE_DIR_TEST+\"*/*.png\")\n",
        "        \n",
        "        for img_path in img_paths:\n",
        "            if self.CLASS_NUMBER==5:\n",
        "                if \"normal\" in img_path:\n",
        "                    x.append(img_path)\n",
        "                    label.append([1,0,0,0,0])\n",
        "                elif \"aom\" in img_path:\n",
        "                    x.append(img_path)\n",
        "                    label.append([0,1,0,0,0])\n",
        "                elif \"csom\" in img_path:\n",
        "                    x.append(img_path)\n",
        "                    label.append([0,0,1,0,0])\n",
        "                elif \"earwax\" in img_path:\n",
        "                    x.append(img_path)\n",
        "                    label.append([0,0,0,1,0])\n",
        "\n",
        "                elif \"other\" in img_path:\n",
        "                        x.append(img_path)\n",
        "                        label.append([0,0,0,0,1])\n",
        "                    \n",
        "            elif self.CLASS_NUMBER==4:\n",
        "                if \"normal\" in img_path:\n",
        "                    x.append(img_path)\n",
        "                    label.append([1,0,0,0])\n",
        "                elif \"aom\" in img_path:\n",
        "                    x.append(img_path)\n",
        "                    label.append([0,1,0,0])\n",
        "                elif \"csom\" in img_path:\n",
        "                    x.append(img_path)\n",
        "                    label.append([0,0,1,0])\n",
        "                elif \"earwax\" in img_path:\n",
        "                    x.append(img_path)\n",
        "                    label.append([0,0,0,1])\n",
        "                \n",
        "        return x, label\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.get_paths_n_labels()[0])\n",
        "    \n",
        "    def get_img(self, img_path):\n",
        "        img = Image.open(img_path)\n",
        "        return np.array(img)\n",
        "    \n",
        "    def augmenting(self, img):\n",
        "        if self.AUG:\n",
        "            augment = Compose([VerticalFlip(p=0.5),\n",
        "                               HorizontalFlip(p=0.5),\n",
        "                               RandomBrightnessContrast(p=0.3),\n",
        "                               ShiftScaleRotate(p=0.5, shift_limit=0.0, scale_limit=0.05, rotate_limit=20)])  \n",
        "        else:\n",
        "            augment = Compose([])  \n",
        "\n",
        "        img = augment(image=img)['image']\n",
        "        return img\n",
        "    \n",
        "    \n",
        "    def resize_and_normalize(self, img):\n",
        "        img = resize(img, self.SHAPE)\n",
        "        return img\n",
        "    \n",
        "    def get_shuffled_data(self):\n",
        "        train_img_paths, train_labels = self.get_train_paths_n_labels()\n",
        "\n",
        "        np.random.seed(self.SEED) \n",
        "        np.random.shuffle(train_img_paths)\n",
        "        \n",
        "        np.random.seed(self.SEED) \n",
        "        np.random.shuffle(train_labels)\n",
        "        \n",
        "        test_img_paths, test_labels = self.get_test_paths_n_labels()\n",
        "\n",
        "        np.random.seed(self.SEED) \n",
        "        np.random.shuffle(test_img_paths)\n",
        "        \n",
        "        np.random.seed(self.SEED) \n",
        "        np.random.shuffle(test_labels)\n",
        "        \n",
        "        return train_img_paths, train_labels, test_img_paths, test_labels\n",
        "        \n",
        "    def split_train_test(self, get):  # get==>{\"train\",\"test\"}\n",
        "        train_img_paths, train_labels, test_img_paths, test_labels = self.get_shuffled_data()\n",
        "        \n",
        "        if get=='train':\n",
        "            return train_img_paths, train_labels\n",
        "        \n",
        "        elif get=='test':\n",
        "            return test_img_paths, test_labels\n",
        "    \n",
        "    def data_generator(self):\n",
        "        img_paths, labels = self.split_train_test(get=\"train\")\n",
        "        \n",
        "        while True:\n",
        "            x = np.empty((self.BATCH_SIZE,)+self.SHAPE, dtype=np.float32)\n",
        "            y = np.empty((self.BATCH_SIZE, 4), dtype=np.float32) # there class=4 or class =5\n",
        "\n",
        "            batch = np.random.choice(self.arr, self.BATCH_SIZE)\n",
        "\n",
        "            for ix, id_ in enumerate(batch):\n",
        "                # x\n",
        "                img_path = img_paths[id_]\n",
        "                img = self.get_img(img_path)\n",
        "                img = self.augmenting(img)\n",
        "                img = self.resize_and_normalize(img)\n",
        "                  \n",
        "                # y \n",
        "                label = labels[id_]\n",
        "             \n",
        "                # Store the values    \n",
        "                x[ix] = img\n",
        "                y[ix] = label\n",
        "\n",
        "            yield x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bJ-aVohsqt1F",
        "outputId": "01a144fa-eb06-40d4-fbfd-aaa307975d0c",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1204
        }
      },
      "source": [
        "dataset = DATASET(SHAPE, 1, range(4), BASE_DIR_TRAIN, BASE_DIR_TEST, SEED, TRAIN_TEST_RATIO, TOTAL_CLASS_NUMBER, augment=AUGMENT_BOOL)\n",
        "\n",
        "for ix, data in enumerate(dataset.data_generator()):\n",
        "    img, y = data\n",
        "    print(img)\n",
        "    print(img.shape)\n",
        "    print(\"-\"*10)\n",
        "    print(y)\n",
        "    print(y.shape)\n",
        "    print(\"-\"*10)\n",
        "    print(img[0,:,:,:].shape)\n",
        "    plt.imshow(img[0,:,:,:])\n",
        "    plt.show()\n",
        "    \n",
        "    if ix==0:\n",
        "        break"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[[0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]]]]\n",
            "(1, 224, 224, 3)\n",
            "----------\n",
            "[[0. 0. 1. 0.]]\n",
            "(1, 4)\n",
            "----------\n",
            "(224, 224, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHzJ+5+xpdkb2AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3MkC15A6vgZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NZygcwwp0Sry",
        "colab": {}
      },
      "source": [
        "# credits: https://stackoverflow.com/questions/43547402/how-to-calculate-f1-macro-in-keras\n",
        "def recall(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Recall metric.\n",
        "    \n",
        "    Only computes a batch-wise average of recall.\n",
        "    \n",
        "    Computes the recall, a metric for multi-label classification of\n",
        "    how many relevant items are selected.\n",
        "    \"\"\"\n",
        "    \n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    \"\"\"Precision metric.\n",
        "    \n",
        "    Only computes a batch-wise average of precision.\n",
        "    \n",
        "    Computes the precision, a metric for multi-label classification of\n",
        "    how many selected items are relevant.\n",
        "    \"\"\"\n",
        "    \n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    precisionx = precision(y_true, y_pred)\n",
        "    recallx = recall(y_true, y_pred)\n",
        "    return 2*((precisionx*recallx)/(precisionx+recallx+K.epsilon()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bBbVuHAfgHnh",
        "colab": {}
      },
      "source": [
        "class SGDRScheduler(Callback):\n",
        "    '''Cosine annealing learning rate scheduler with periodic restarts.\n",
        "    # Usage\n",
        "        ```python\n",
        "            schedule = SGDRScheduler(min_lr=1e-5,\n",
        "                                     max_lr=1e-2,\n",
        "                                     steps_per_epoch=np.ceil(epoch_size/batch_size),\n",
        "                                     lr_decay=0.9,\n",
        "                                     cycle_length=5,\n",
        "                                     mult_factor=1.5)\n",
        "            model.fit(X_train, Y_train, epochs=100, callbacks=[schedule])\n",
        "        ```\n",
        "    # Arguments\n",
        "        min_lr: The lower bound of the learning rate range for the experiment.\n",
        "        max_lr: The upper bound of the learning rate range for the experiment.\n",
        "        steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size/batch_size)`. \n",
        "        lr_decay: Reduce the max_lr after the completion of each cycle.\n",
        "                  Ex. To reduce the max_lr by 20% after each cycle, set this value to 0.8.\n",
        "        cycle_length: Initial number of epochs in a cycle.\n",
        "        mult_factor: Scale epochs_to_restart after each full cycle completion.\n",
        "    # References\n",
        "        Blog post: jeremyjordan.me/nn-learning-rate\n",
        "        Original paper: http://arxiv.org/abs/1608.03983\n",
        "    '''\n",
        "    def __init__(self,\n",
        "                 min_lr,\n",
        "                 max_lr,\n",
        "                 steps_per_epoch,\n",
        "                 lr_decay=1,\n",
        "                 cycle_length=10,\n",
        "                 mult_factor=2):\n",
        "\n",
        "        self.min_lr = min_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.lr_decay = lr_decay\n",
        "\n",
        "        self.batch_since_restart = 0\n",
        "        self.next_restart = cycle_length\n",
        "\n",
        "        self.steps_per_epoch = steps_per_epoch\n",
        "\n",
        "        self.cycle_length = cycle_length\n",
        "        self.mult_factor = mult_factor\n",
        "\n",
        "        self.history = {}\n",
        "\n",
        "    def clr(self):\n",
        "        '''Calculate the learning rate.'''\n",
        "        fraction_to_restart = self.batch_since_restart / (self.steps_per_epoch * self.cycle_length)\n",
        "        lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(fraction_to_restart * np.pi))\n",
        "        return lr\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        '''Initialize the learning rate to the minimum value at the start of training.'''\n",
        "        logs = logs or {}\n",
        "        K.set_value(self.model.optimizer.lr, self.max_lr)\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        '''Record previous batch statistics and update the learning rate.'''\n",
        "        logs = logs or {}\n",
        "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "\n",
        "        self.batch_since_restart += 1\n",
        "        K.set_value(self.model.optimizer.lr, self.clr())\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        '''Check for end of current cycle, apply restarts when necessary.'''\n",
        "        if epoch + 1 == self.next_restart:\n",
        "            self.batch_since_restart = 0\n",
        "            self.cycle_length = np.ceil(self.cycle_length * self.mult_factor)\n",
        "            self.next_restart += self.cycle_length\n",
        "            self.max_lr *= self.lr_decay\n",
        "            self.best_weights = self.model.get_weights()\n",
        "\n",
        "    def on_train_end(self, logs={}):\n",
        "        '''Set weights to the values from the end of the most recent cycle for best performance.'''\n",
        "        self.model.set_weights(self.best_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IvvEuAAKfkV5",
        "colab": {}
      },
      "source": [
        "# copied from https://github.com/kobiso/CBAM-keras/blob/master/models/attention_module.py\n",
        "def cbam_block(cbam_feature, ratio=8):\n",
        "    \"\"\"Contains the implementation of Convolutional Block Attention Module(CBAM) block.\n",
        "    As described in https://arxiv.org/abs/1807.06521.\n",
        "    \"\"\"\n",
        "    \n",
        "    cbam_feature = channel_attention(cbam_feature, ratio)\n",
        "    cbam_feature = spatial_attention(cbam_feature)\n",
        "    return cbam_feature\n",
        "\n",
        "def channel_attention(input_feature, ratio=8):\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "    channel = input_feature._keras_shape[channel_axis]\n",
        "    \n",
        "    shared_layer_one = Dense(channel//ratio,\n",
        "                             activation='relu',\n",
        "                             kernel_initializer='he_normal',\n",
        "                             use_bias=True,\n",
        "                             bias_initializer='zeros')\n",
        "    shared_layer_two = Dense(channel,\n",
        "                             kernel_initializer='he_normal',\n",
        "                             use_bias=True,\n",
        "                             bias_initializer='zeros')\n",
        "    \n",
        "    avg_pool = GlobalAveragePooling2D()(input_feature)    \n",
        "    avg_pool = Reshape((1,1,channel))(avg_pool)\n",
        "    assert avg_pool._keras_shape[1:] == (1,1,channel)\n",
        "    avg_pool = shared_layer_one(avg_pool)\n",
        "    assert avg_pool._keras_shape[1:] == (1,1,channel//ratio)\n",
        "    avg_pool = shared_layer_two(avg_pool)\n",
        "    assert avg_pool._keras_shape[1:] == (1,1,channel)\n",
        "    \n",
        "    max_pool = GlobalMaxPooling2D()(input_feature)\n",
        "    max_pool = Reshape((1,1,channel))(max_pool)\n",
        "    assert max_pool._keras_shape[1:] == (1,1,channel)\n",
        "    max_pool = shared_layer_one(max_pool)\n",
        "    assert max_pool._keras_shape[1:] == (1,1,channel//ratio)\n",
        "    max_pool = shared_layer_two(max_pool)\n",
        "    assert max_pool._keras_shape[1:] == (1,1,channel)\n",
        "    \n",
        "    cbam_feature = Add()([avg_pool,max_pool])\n",
        "    cbam_feature = Activation('sigmoid')(cbam_feature)\n",
        "\n",
        "    if K.image_data_format() == \"channels_first\":\n",
        "        cbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
        "    \n",
        "    return multiply([input_feature, cbam_feature])\n",
        "\n",
        "def spatial_attention(input_feature):\n",
        "    kernel_size = 7\n",
        "    \n",
        "    if K.image_data_format() == \"channels_first\":\n",
        "        channel = input_feature._keras_shape[1]\n",
        "        cbam_feature = Permute((2,3,1))(input_feature)\n",
        "    else:\n",
        "        channel = input_feature._keras_shape[-1]\n",
        "        cbam_feature = input_feature\n",
        "    \n",
        "    avg_pool = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(cbam_feature)\n",
        "    assert avg_pool._keras_shape[-1] == 1\n",
        "    max_pool = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(cbam_feature)\n",
        "    assert max_pool._keras_shape[-1] == 1\n",
        "    concat = Concatenate(axis=3)([avg_pool, max_pool])\n",
        "    assert concat._keras_shape[-1] == 2\n",
        "    cbam_feature = Conv2D(filters = 1,\n",
        "                    kernel_size=kernel_size,\n",
        "                    strides=1,\n",
        "                    padding='same',\n",
        "                    activation='sigmoid',\n",
        "                    kernel_initializer='he_normal',\n",
        "                    use_bias=False)(concat)\t\n",
        "    assert cbam_feature._keras_shape[-1] == 1\n",
        "    \n",
        "    if K.image_data_format() == \"channels_first\":\n",
        "        cbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
        "        \n",
        "    return multiply([input_feature, cbam_feature])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F4Ndx2vm6NZ3",
        "colab": {}
      },
      "source": [
        "# copied from https://gist.github.com/mjdietzx/5319e42637ed7ef095d430cb5c5e8c64\n",
        "def residual_block(y, nb_channels, _strides=(1, 1), _project_shortcut=False):\n",
        "    shortcut = y\n",
        "\n",
        "    # down-sampling is performed with a stride of 2\n",
        "    y = Conv2D(nb_channels, kernel_size=(3, 3), strides=_strides, padding='same')(y)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = LeakyReLU()(y)\n",
        "\n",
        "    y = Conv2D(nb_channels, kernel_size=(3, 3), strides=(1, 1), padding='same')(y)\n",
        "    y = BatchNormalization()(y)\n",
        "\n",
        "    # identity shortcuts used directly when the input and output are of the same dimensions\n",
        "    if _project_shortcut or _strides != (1, 1):\n",
        "        # when the dimensions increase projection shortcut is used to match dimensions (done by 1×1 convolutions)\n",
        "        # when the shortcuts go across feature maps of two sizes, they are performed with a stride of 2\n",
        "        shortcut = Conv2D(nb_channels, kernel_size=(1, 1), strides=_strides, padding='same')(shortcut)\n",
        "        shortcut = BatchNormalization()(shortcut)\n",
        "\n",
        "    y = add([shortcut, y])\n",
        "    y = LeakyReLU()(y)\n",
        "\n",
        "    return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EVUWz9lzfm6Y",
        "colab": {}
      },
      "source": [
        "def create_model():\n",
        "    \n",
        "    dropRate = 0.3\n",
        "    \n",
        "    init = Input(SHAPE)\n",
        "    x = Conv2D(32, (3, 3), activation=None, padding='same')(init) \n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(32, (3, 3), activation=None, padding='same')(x) \n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x1 = MaxPooling2D((2,2))(x)\n",
        "    \n",
        "    x = Conv2D(64, (3, 3), activation=None, padding='same')(x1)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = cbam_block(x)\n",
        "    x = residual_block(x, 64)\n",
        "    x2 = MaxPooling2D((2,2))(x)\n",
        "    \n",
        "    x = Conv2D(128, (3, 3), activation=None, padding='same')(x2)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = cbam_block(x)\n",
        "    x = residual_block(x, 128)\n",
        "    x3 = MaxPooling2D((2,2))(x)\n",
        "    \n",
        "    ginp1 = UpSampling2D(size=(2, 2), interpolation='bilinear')(x1)\n",
        "    ginp2 = UpSampling2D(size=(4, 4), interpolation='bilinear')(x2)\n",
        "    ginp3 = UpSampling2D(size=(8, 8), interpolation='bilinear')(x3)\n",
        "    \n",
        "    hypercolumn = Concatenate()([ginp1, ginp2, ginp3]) \n",
        "    gap = GlobalAveragePooling2D()(hypercolumn)\n",
        "\n",
        "    x = Dense(256, activation=None)(gap)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dropout(dropRate)(x)\n",
        "    \n",
        "    x = Dense(256, activation=None)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    \n",
        "    y = Dense(TOTAL_CLASS_NUMBER, activation='softmax')(x)\n",
        "   \n",
        "    model = Model(init, y)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w2V3AUW7fm-n",
        "outputId": "1f5c2f5a-1f40-4ae7-9c2b-50af00f43662",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3386
        }
      },
      "source": [
        "model = create_model()\n",
        "model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2241: The name tf.image.resize_bilinear is deprecated. Please use tf.compat.v1.image.resize_bilinear instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 224, 224, 32) 896         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 224, 224, 32) 128         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 224, 224, 32) 0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 224, 224, 32) 9248        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 224, 224, 32) 128         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 224, 224, 32) 0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 112, 112, 32) 0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 112, 112, 64) 18496       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 112, 112, 64) 256         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 112, 112, 64) 0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_1 (Glo (None, 64)           0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d_1 (GlobalM (None, 64)           0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "reshape_1 (Reshape)             (None, 1, 1, 64)     0           global_average_pooling2d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "reshape_2 (Reshape)             (None, 1, 1, 64)     0           global_max_pooling2d_1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1, 1, 8)      520         reshape_1[0][0]                  \n",
            "                                                                 reshape_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1, 1, 64)     576         dense_1[0][0]                    \n",
            "                                                                 dense_1[1][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 1, 1, 64)     0           dense_2[0][0]                    \n",
            "                                                                 dense_2[1][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 1, 1, 64)     0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "multiply_1 (Multiply)           (None, 112, 112, 64) 0           activation_3[0][0]               \n",
            "                                                                 activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 112, 112, 1)  0           multiply_1[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 112, 112, 1)  0           multiply_1[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 112, 112, 2)  0           lambda_1[0][0]                   \n",
            "                                                                 lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 112, 112, 1)  98          concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "multiply_2 (Multiply)           (None, 112, 112, 64) 0           multiply_1[0][0]                 \n",
            "                                                                 conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 112, 112, 64) 36928       multiply_2[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 112, 112, 64) 256         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)       (None, 112, 112, 64) 0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 112, 112, 64) 36928       leaky_re_lu_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 112, 112, 64) 256         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 112, 112, 64) 0           multiply_2[0][0]                 \n",
            "                                                                 batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)       (None, 112, 112, 64) 0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 56, 56, 64)   0           leaky_re_lu_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 56, 56, 128)  73856       max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 56, 56, 128)  512         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 56, 56, 128)  0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_2 (Glo (None, 128)          0           activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d_2 (GlobalM (None, 128)          0           activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "reshape_3 (Reshape)             (None, 1, 1, 128)    0           global_average_pooling2d_2[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "reshape_4 (Reshape)             (None, 1, 1, 128)    0           global_max_pooling2d_2[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 1, 1, 16)     2064        reshape_3[0][0]                  \n",
            "                                                                 reshape_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 1, 1, 128)    2176        dense_3[0][0]                    \n",
            "                                                                 dense_3[1][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 1, 1, 128)    0           dense_4[0][0]                    \n",
            "                                                                 dense_4[1][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 1, 1, 128)    0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "multiply_3 (Multiply)           (None, 56, 56, 128)  0           activation_5[0][0]               \n",
            "                                                                 activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lambda_3 (Lambda)               (None, 56, 56, 1)    0           multiply_3[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_4 (Lambda)               (None, 56, 56, 1)    0           multiply_3[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 56, 56, 2)    0           lambda_3[0][0]                   \n",
            "                                                                 lambda_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 56, 56, 1)    98          concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "multiply_4 (Multiply)           (None, 56, 56, 128)  0           multiply_3[0][0]                 \n",
            "                                                                 conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 56, 56, 128)  147584      multiply_4[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 56, 56, 128)  512         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)       (None, 56, 56, 128)  0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 56, 56, 128)  147584      leaky_re_lu_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 56, 56, 128)  512         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 56, 56, 128)  0           multiply_4[0][0]                 \n",
            "                                                                 batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)       (None, 56, 56, 128)  0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 28, 28, 128)  0           leaky_re_lu_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2D)  (None, 224, 224, 32) 0           max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2D)  (None, 224, 224, 64) 0           max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_3 (UpSampling2D)  (None, 224, 224, 128 0           max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 224, 224, 224 0           up_sampling2d_1[0][0]            \n",
            "                                                                 up_sampling2d_2[0][0]            \n",
            "                                                                 up_sampling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_3 (Glo (None, 224)          0           concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 256)          57600       global_average_pooling2d_3[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 256)          1024        dense_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 256)          0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 256)          0           activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 256)          65792       dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 256)          1024        dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 256)          0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 4)            1028        activation_8[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 606,080\n",
            "Trainable params: 603,776\n",
            "Non-trainable params: 2,304\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MSHMA1gtMQ6e",
        "outputId": "a10cb540-604e-4040-d44b-eaff655d9909",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 23582
        }
      },
      "source": [
        "kf = KFold(n_splits=N_SPLITS, random_state=SEED, shuffle=True)\n",
        "for ix, (train_index, test_index) in enumerate(kf.split(range(len(dataset.split_train_test(\"train\")[0])))):\n",
        "                                               \n",
        "    tg = DATASET(SHAPE, BATCH_SIZE, train_index, BASE_DIR_TRAIN, BASE_DIR_TEST, SEED, TRAIN_TEST_RATIO, TOTAL_CLASS_NUMBER, augment=AUGMENT_BOOL)\n",
        "    vg = DATASET(SHAPE, BATCH_SIZE, test_index , BASE_DIR_TRAIN, BASE_DIR_TEST, SEED, TRAIN_TEST_RATIO, TOTAL_CLASS_NUMBER, augment=False)\n",
        "        \n",
        "    schedule = SGDRScheduler(min_lr=1e-6,\n",
        "                             max_lr=1e-3,\n",
        "                             steps_per_epoch=np.ceil(EPOCHS/BATCH_SIZE),\n",
        "                             lr_decay=0.9,\n",
        "                             cycle_length=10,\n",
        "                             mult_factor=2.)\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-3), metrics=[precision, recall, f1, 'acc'])\n",
        "\n",
        "    model_ckpt = \"BREASTNET_FOLD_\"+str(ix)+\".h5\"\n",
        "    callbacks = [ModelCheckpoint(model_ckpt, monitor='val_loss', mode='min', verbose=1, save_best_only=True, save_weights_only=False),\n",
        "                 TensorBoard(log_dir='./log_'+str(ix), update_freq='batch'), \n",
        "                 schedule] \n",
        "    model.fit_generator(tg.data_generator(),\n",
        "                        steps_per_epoch=len(train_index)//BATCH_SIZE,\n",
        "                        epochs=EPOCHS,\n",
        "                        verbose=2,\n",
        "                        validation_data=vg.data_generator(),\n",
        "                        validation_steps=len(test_index)//BATCH_SIZE,\n",
        "                        callbacks=callbacks)\n",
        "                                             \n",
        "  \n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/100\n",
            " - 199s - loss: 1.0861 - precision: 0.6509 - recall: 0.4291 - f1: 0.5133 - acc: 0.5829 - val_loss: 1.1646 - val_precision: 0.6456 - val_recall: 0.3385 - val_f1: 0.4436 - val_acc: 0.4792\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.16459, saving model to BREASTNET_FOLD_0.h5\n",
            "Epoch 2/100\n",
            " - 93s - loss: 0.8412 - precision: 0.7567 - recall: 0.6058 - f1: 0.6716 - acc: 0.6983 - val_loss: 0.8962 - val_precision: 0.7442 - val_recall: 0.5729 - val_f1: 0.6473 - val_acc: 0.6927\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.16459 to 0.89621, saving model to BREASTNET_FOLD_0.h5\n",
            "Epoch 3/100\n",
            " - 63s - loss: 0.8477 - precision: 0.7341 - recall: 0.6310 - f1: 0.6785 - acc: 0.7019 - val_loss: 1.9254 - val_precision: 0.3124 - val_recall: 0.2604 - val_f1: 0.2840 - val_acc: 0.3229\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.89621\n",
            "Epoch 4/100\n",
            " - 46s - loss: 0.8410 - precision: 0.7320 - recall: 0.6623 - f1: 0.6951 - acc: 0.6923 - val_loss: 2.5401 - val_precision: 0.2207 - val_recall: 0.2031 - val_f1: 0.2115 - val_acc: 0.2188\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.89621\n",
            "Epoch 5/100\n",
            " - 45s - loss: 0.8142 - precision: 0.7602 - recall: 0.6623 - f1: 0.7075 - acc: 0.7067 - val_loss: 1.1039 - val_precision: 0.6489 - val_recall: 0.5781 - val_f1: 0.6115 - val_acc: 0.6302\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.89621\n",
            "Epoch 6/100\n",
            " - 43s - loss: 0.7874 - precision: 0.7633 - recall: 0.6743 - f1: 0.7158 - acc: 0.7284 - val_loss: 1.9126 - val_precision: 0.4022 - val_recall: 0.3073 - val_f1: 0.3476 - val_acc: 0.4062\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.89621\n",
            "Epoch 7/100\n",
            " - 42s - loss: 0.7603 - precision: 0.7689 - recall: 0.6839 - f1: 0.7237 - acc: 0.7332 - val_loss: 1.8209 - val_precision: 0.3271 - val_recall: 0.2708 - val_f1: 0.2958 - val_acc: 0.3229\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.89621\n",
            "Epoch 8/100\n",
            " - 42s - loss: 0.7660 - precision: 0.7608 - recall: 0.7079 - f1: 0.7333 - acc: 0.7380 - val_loss: 1.4732 - val_precision: 0.5344 - val_recall: 0.4010 - val_f1: 0.4566 - val_acc: 0.5260\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.89621\n",
            "Epoch 9/100\n",
            " - 41s - loss: 0.7635 - precision: 0.7586 - recall: 0.6959 - f1: 0.7256 - acc: 0.7212 - val_loss: 2.5783 - val_precision: 0.2329 - val_recall: 0.2135 - val_f1: 0.2228 - val_acc: 0.2604\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.89621\n",
            "Epoch 10/100\n",
            " - 41s - loss: 0.7628 - precision: 0.7592 - recall: 0.6755 - f1: 0.7148 - acc: 0.7260 - val_loss: 1.5002 - val_precision: 0.4422 - val_recall: 0.3958 - val_f1: 0.4176 - val_acc: 0.4688\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.89621\n",
            "Epoch 11/100\n",
            " - 40s - loss: 0.8077 - precision: 0.7518 - recall: 0.6755 - f1: 0.7114 - acc: 0.7127 - val_loss: 1.4688 - val_precision: 0.4010 - val_recall: 0.2656 - val_f1: 0.3195 - val_acc: 0.3281\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.89621\n",
            "Epoch 12/100\n",
            " - 41s - loss: 0.6397 - precision: 0.8102 - recall: 0.7344 - f1: 0.7701 - acc: 0.7740 - val_loss: 2.8664 - val_precision: 0.2235 - val_recall: 0.2083 - val_f1: 0.2156 - val_acc: 0.2292\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.89621\n",
            "Epoch 13/100\n",
            " - 40s - loss: 0.7067 - precision: 0.7767 - recall: 0.7175 - f1: 0.7459 - acc: 0.7560 - val_loss: 1.8142 - val_precision: 0.3589 - val_recall: 0.2917 - val_f1: 0.3215 - val_acc: 0.3229\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.89621\n",
            "Epoch 14/100\n",
            " - 41s - loss: 0.7212 - precision: 0.7772 - recall: 0.7115 - f1: 0.7428 - acc: 0.7500 - val_loss: 1.5771 - val_precision: 0.3232 - val_recall: 0.2240 - val_f1: 0.2641 - val_acc: 0.3542\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.89621\n",
            "Epoch 15/100\n",
            " - 41s - loss: 0.6163 - precision: 0.8112 - recall: 0.7356 - f1: 0.7710 - acc: 0.7716 - val_loss: 1.4005 - val_precision: 0.5141 - val_recall: 0.4323 - val_f1: 0.4693 - val_acc: 0.5312\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.89621\n",
            "Epoch 16/100\n",
            " - 42s - loss: 0.6591 - precision: 0.8039 - recall: 0.7236 - f1: 0.7613 - acc: 0.7716 - val_loss: 1.8763 - val_precision: 0.4184 - val_recall: 0.3802 - val_f1: 0.3983 - val_acc: 0.4010\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.89621\n",
            "Epoch 17/100\n",
            " - 43s - loss: 0.7064 - precision: 0.7689 - recall: 0.6923 - f1: 0.7282 - acc: 0.7236 - val_loss: 1.0857 - val_precision: 0.6964 - val_recall: 0.6302 - val_f1: 0.6614 - val_acc: 0.6771\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.89621\n",
            "Epoch 18/100\n",
            " - 43s - loss: 0.7256 - precision: 0.7833 - recall: 0.6923 - f1: 0.7345 - acc: 0.7368 - val_loss: 2.0605 - val_precision: 0.4676 - val_recall: 0.4531 - val_f1: 0.4603 - val_acc: 0.4531\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.89621\n",
            "Epoch 19/100\n",
            " - 42s - loss: 0.6044 - precision: 0.8106 - recall: 0.7452 - f1: 0.7762 - acc: 0.7861 - val_loss: 1.3679 - val_precision: 0.5802 - val_recall: 0.5469 - val_f1: 0.5631 - val_acc: 0.5677\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.89621\n",
            "Epoch 20/100\n",
            " - 42s - loss: 0.6493 - precision: 0.7964 - recall: 0.7236 - f1: 0.7581 - acc: 0.7656 - val_loss: 1.1222 - val_precision: 0.5935 - val_recall: 0.5469 - val_f1: 0.5691 - val_acc: 0.5729\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.89621\n",
            "Epoch 21/100\n",
            " - 42s - loss: 0.6039 - precision: 0.8145 - recall: 0.7224 - f1: 0.7653 - acc: 0.7764 - val_loss: 0.9969 - val_precision: 0.6937 - val_recall: 0.5990 - val_f1: 0.6427 - val_acc: 0.6667\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.89621\n",
            "Epoch 22/100\n",
            " - 41s - loss: 0.5769 - precision: 0.8283 - recall: 0.7644 - f1: 0.7947 - acc: 0.7969 - val_loss: 0.8545 - val_precision: 0.7560 - val_recall: 0.6458 - val_f1: 0.6964 - val_acc: 0.7188\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.89621 to 0.85453, saving model to BREASTNET_FOLD_0.h5\n",
            "Epoch 23/100\n",
            " - 41s - loss: 0.6102 - precision: 0.8276 - recall: 0.7548 - f1: 0.7892 - acc: 0.7921 - val_loss: 1.0297 - val_precision: 0.7114 - val_recall: 0.6042 - val_f1: 0.6533 - val_acc: 0.6927\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.85453\n",
            "Epoch 24/100\n",
            " - 42s - loss: 0.6155 - precision: 0.8088 - recall: 0.7632 - f1: 0.7853 - acc: 0.7825 - val_loss: 0.6847 - val_precision: 0.8362 - val_recall: 0.6562 - val_f1: 0.7348 - val_acc: 0.7604\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.85453 to 0.68472, saving model to BREASTNET_FOLD_0.h5\n",
            "Epoch 25/100\n",
            " - 41s - loss: 0.6360 - precision: 0.7889 - recall: 0.7284 - f1: 0.7572 - acc: 0.7560 - val_loss: 0.7300 - val_precision: 0.7626 - val_recall: 0.6354 - val_f1: 0.6928 - val_acc: 0.7188\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.68472\n",
            "Epoch 26/100\n",
            " - 41s - loss: 0.6585 - precision: 0.8112 - recall: 0.7308 - f1: 0.7685 - acc: 0.7740 - val_loss: 0.6549 - val_precision: 0.7835 - val_recall: 0.7135 - val_f1: 0.7467 - val_acc: 0.7656\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.68472 to 0.65491, saving model to BREASTNET_FOLD_0.h5\n",
            "Epoch 27/100\n",
            " - 41s - loss: 0.5920 - precision: 0.8337 - recall: 0.7524 - f1: 0.7908 - acc: 0.7861 - val_loss: 0.9052 - val_precision: 0.6898 - val_recall: 0.6719 - val_f1: 0.6807 - val_acc: 0.6823\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.65491\n",
            "Epoch 28/100\n",
            " - 41s - loss: 0.5196 - precision: 0.8516 - recall: 0.7800 - f1: 0.8137 - acc: 0.8185 - val_loss: 0.7819 - val_precision: 0.7719 - val_recall: 0.6823 - val_f1: 0.7242 - val_acc: 0.7448\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.65491\n",
            "Epoch 29/100\n",
            " - 41s - loss: 0.6235 - precision: 0.8113 - recall: 0.7416 - f1: 0.7745 - acc: 0.7800 - val_loss: 0.8194 - val_precision: 0.7608 - val_recall: 0.6406 - val_f1: 0.6955 - val_acc: 0.7188\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.65491\n",
            "Epoch 30/100\n",
            " - 41s - loss: 0.6291 - precision: 0.8194 - recall: 0.7440 - f1: 0.7797 - acc: 0.7825 - val_loss: 1.3656 - val_precision: 0.5690 - val_recall: 0.4323 - val_f1: 0.4911 - val_acc: 0.4583\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.65491\n",
            "Epoch 31/100\n",
            " - 41s - loss: 0.5581 - precision: 0.8336 - recall: 0.7596 - f1: 0.7946 - acc: 0.8053 - val_loss: 1.5971 - val_precision: 0.4549 - val_recall: 0.2969 - val_f1: 0.3591 - val_acc: 0.4010\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.65491\n",
            "Epoch 32/100\n",
            " - 42s - loss: 0.5838 - precision: 0.8143 - recall: 0.7632 - f1: 0.7878 - acc: 0.7909 - val_loss: 1.0566 - val_precision: 0.7684 - val_recall: 0.7240 - val_f1: 0.7454 - val_acc: 0.7500\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.65491\n",
            "Epoch 33/100\n",
            " - 41s - loss: 0.5784 - precision: 0.8151 - recall: 0.7512 - f1: 0.7816 - acc: 0.7921 - val_loss: 0.8295 - val_precision: 0.7436 - val_recall: 0.6615 - val_f1: 0.7001 - val_acc: 0.7240\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.65491\n",
            "Epoch 34/100\n",
            " - 42s - loss: 0.5934 - precision: 0.8222 - recall: 0.7488 - f1: 0.7835 - acc: 0.7837 - val_loss: 0.6982 - val_precision: 0.8087 - val_recall: 0.7656 - val_f1: 0.7864 - val_acc: 0.7917\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.65491\n",
            "Epoch 35/100\n",
            " - 41s - loss: 0.5043 - precision: 0.8498 - recall: 0.8017 - f1: 0.8249 - acc: 0.8269 - val_loss: 0.6682 - val_precision: 0.8059 - val_recall: 0.7135 - val_f1: 0.7569 - val_acc: 0.7708\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.65491\n",
            "Epoch 36/100\n",
            " - 42s - loss: 0.5789 - precision: 0.8287 - recall: 0.7512 - f1: 0.7880 - acc: 0.7957 - val_loss: 0.7747 - val_precision: 0.8335 - val_recall: 0.7500 - val_f1: 0.7894 - val_acc: 0.7969\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.65491\n",
            "Epoch 37/100\n",
            " - 42s - loss: 0.5586 - precision: 0.8216 - recall: 0.7572 - f1: 0.7878 - acc: 0.7909 - val_loss: 0.7798 - val_precision: 0.7806 - val_recall: 0.7188 - val_f1: 0.7482 - val_acc: 0.7500\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.65491\n",
            "Epoch 38/100\n",
            " - 43s - loss: 0.5268 - precision: 0.8355 - recall: 0.7680 - f1: 0.8001 - acc: 0.8053 - val_loss: 0.7293 - val_precision: 0.8174 - val_recall: 0.7760 - val_f1: 0.7960 - val_acc: 0.7812\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.65491\n",
            "Epoch 39/100\n",
            " - 43s - loss: 0.5307 - precision: 0.8344 - recall: 0.7656 - f1: 0.7982 - acc: 0.8041 - val_loss: 0.8044 - val_precision: 0.7927 - val_recall: 0.7188 - val_f1: 0.7538 - val_acc: 0.7344\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.65491\n",
            "Epoch 40/100\n",
            " - 43s - loss: 0.4932 - precision: 0.8502 - recall: 0.7800 - f1: 0.8131 - acc: 0.8245 - val_loss: 0.8744 - val_precision: 0.7522 - val_recall: 0.6667 - val_f1: 0.7067 - val_acc: 0.6719\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.65491\n",
            "Epoch 41/100\n",
            " - 42s - loss: 0.5585 - precision: 0.8248 - recall: 0.7596 - f1: 0.7907 - acc: 0.7885 - val_loss: 1.0382 - val_precision: 0.6771 - val_recall: 0.6042 - val_f1: 0.6382 - val_acc: 0.6250\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.65491\n",
            "Epoch 42/100\n",
            " - 42s - loss: 0.5732 - precision: 0.8390 - recall: 0.7716 - f1: 0.8035 - acc: 0.8077 - val_loss: 1.3586 - val_precision: 0.4718 - val_recall: 0.3177 - val_f1: 0.3789 - val_acc: 0.4219\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.65491\n",
            "Epoch 43/100\n",
            " - 42s - loss: 0.5802 - precision: 0.8217 - recall: 0.7428 - f1: 0.7798 - acc: 0.7897 - val_loss: 0.9893 - val_precision: 0.6253 - val_recall: 0.5208 - val_f1: 0.5683 - val_acc: 0.6042\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.65491\n",
            "Epoch 44/100\n",
            " - 42s - loss: 0.5794 - precision: 0.8328 - recall: 0.7464 - f1: 0.7868 - acc: 0.7969 - val_loss: 1.2946 - val_precision: 0.6089 - val_recall: 0.4375 - val_f1: 0.5091 - val_acc: 0.5521\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.65491\n",
            "Epoch 45/100\n",
            " - 41s - loss: 0.5659 - precision: 0.8246 - recall: 0.7608 - f1: 0.7913 - acc: 0.7981 - val_loss: 0.8525 - val_precision: 0.7289 - val_recall: 0.6042 - val_f1: 0.6606 - val_acc: 0.6615\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.65491\n",
            "Epoch 46/100\n",
            " - 41s - loss: 0.5340 - precision: 0.8437 - recall: 0.7728 - f1: 0.8066 - acc: 0.8113 - val_loss: 0.8059 - val_precision: 0.7681 - val_recall: 0.6406 - val_f1: 0.6985 - val_acc: 0.7552\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.65491\n",
            "Epoch 47/100\n",
            " - 41s - loss: 0.5107 - precision: 0.8467 - recall: 0.7825 - f1: 0.8130 - acc: 0.8209 - val_loss: 0.7455 - val_precision: 0.8108 - val_recall: 0.7396 - val_f1: 0.7729 - val_acc: 0.7500\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.65491\n",
            "Epoch 48/100\n",
            " - 41s - loss: 0.4758 - precision: 0.8611 - recall: 0.7969 - f1: 0.8276 - acc: 0.8317 - val_loss: 0.7726 - val_precision: 0.8257 - val_recall: 0.7656 - val_f1: 0.7945 - val_acc: 0.7969\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.65491\n",
            "Epoch 49/100\n",
            " - 43s - loss: 0.5125 - precision: 0.8322 - recall: 0.7692 - f1: 0.7992 - acc: 0.8089 - val_loss: 0.5975 - val_precision: 0.8392 - val_recall: 0.7500 - val_f1: 0.7917 - val_acc: 0.8125\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.65491 to 0.59754, saving model to BREASTNET_FOLD_0.h5\n",
            "Epoch 50/100\n",
            " - 43s - loss: 0.4644 - precision: 0.8701 - recall: 0.7993 - f1: 0.8328 - acc: 0.8389 - val_loss: 0.5906 - val_precision: 0.8690 - val_recall: 0.7604 - val_f1: 0.8111 - val_acc: 0.8385\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.59754 to 0.59057, saving model to BREASTNET_FOLD_0.h5\n",
            "Epoch 51/100\n",
            " - 42s - loss: 0.4888 - precision: 0.8540 - recall: 0.7945 - f1: 0.8228 - acc: 0.8293 - val_loss: 0.8059 - val_precision: 0.7732 - val_recall: 0.6719 - val_f1: 0.7186 - val_acc: 0.7135\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.59057\n",
            "Epoch 52/100\n",
            " - 43s - loss: 0.5293 - precision: 0.8371 - recall: 0.7800 - f1: 0.8074 - acc: 0.8125 - val_loss: 0.8653 - val_precision: 0.7804 - val_recall: 0.6719 - val_f1: 0.7216 - val_acc: 0.7031\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.59057\n",
            "Epoch 53/100\n",
            " - 42s - loss: 0.5603 - precision: 0.8398 - recall: 0.7608 - f1: 0.7980 - acc: 0.7945 - val_loss: 0.8535 - val_precision: 0.7600 - val_recall: 0.6771 - val_f1: 0.7160 - val_acc: 0.7135\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.59057\n",
            "Epoch 54/100\n",
            " - 42s - loss: 0.4917 - precision: 0.8415 - recall: 0.7752 - f1: 0.8068 - acc: 0.8293 - val_loss: 1.9389 - val_precision: 0.3513 - val_recall: 0.2812 - val_f1: 0.3124 - val_acc: 0.3021\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.59057\n",
            "Epoch 55/100\n",
            " - 42s - loss: 0.5642 - precision: 0.8022 - recall: 0.7488 - f1: 0.7743 - acc: 0.7716 - val_loss: 1.0546 - val_precision: 0.6360 - val_recall: 0.5312 - val_f1: 0.5780 - val_acc: 0.6146\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.59057\n",
            "Epoch 56/100\n",
            " - 42s - loss: 0.5354 - precision: 0.8289 - recall: 0.7608 - f1: 0.7932 - acc: 0.8029 - val_loss: 1.7777 - val_precision: 0.4295 - val_recall: 0.4167 - val_f1: 0.4230 - val_acc: 0.4219\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.59057\n",
            "Epoch 57/100\n",
            " - 43s - loss: 0.5476 - precision: 0.8491 - recall: 0.7608 - f1: 0.8023 - acc: 0.8065 - val_loss: 1.0081 - val_precision: 0.6890 - val_recall: 0.6667 - val_f1: 0.6776 - val_acc: 0.6875\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.59057\n",
            "Epoch 58/100\n",
            " - 42s - loss: 0.4768 - precision: 0.8518 - recall: 0.7861 - f1: 0.8175 - acc: 0.8233 - val_loss: 0.9312 - val_precision: 0.7327 - val_recall: 0.6406 - val_f1: 0.6833 - val_acc: 0.6823\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.59057\n",
            "Epoch 59/100\n",
            " - 43s - loss: 0.4921 - precision: 0.8626 - recall: 0.8077 - f1: 0.8341 - acc: 0.8377 - val_loss: 0.9038 - val_precision: 0.7390 - val_recall: 0.6354 - val_f1: 0.6833 - val_acc: 0.6667\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.59057\n",
            "Epoch 60/100\n",
            " - 42s - loss: 0.5186 - precision: 0.8398 - recall: 0.7752 - f1: 0.8059 - acc: 0.8113 - val_loss: 0.7600 - val_precision: 0.7904 - val_recall: 0.7083 - val_f1: 0.7470 - val_acc: 0.7396\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.59057\n",
            "Epoch 61/100\n",
            " - 43s - loss: 0.4380 - precision: 0.8773 - recall: 0.8089 - f1: 0.8415 - acc: 0.8486 - val_loss: 0.8361 - val_precision: 0.7639 - val_recall: 0.6927 - val_f1: 0.7265 - val_acc: 0.7344\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.59057\n",
            "Epoch 62/100\n",
            " - 42s - loss: 0.4281 - precision: 0.8875 - recall: 0.8089 - f1: 0.8462 - acc: 0.8546 - val_loss: 0.7367 - val_precision: 0.8164 - val_recall: 0.7396 - val_f1: 0.7760 - val_acc: 0.7396\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.59057\n",
            "Epoch 63/100\n",
            " - 42s - loss: 0.4612 - precision: 0.8704 - recall: 0.8017 - f1: 0.8345 - acc: 0.8353 - val_loss: 0.7580 - val_precision: 0.7461 - val_recall: 0.6875 - val_f1: 0.7156 - val_acc: 0.7188\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.59057\n",
            "Epoch 64/100\n",
            " - 42s - loss: 0.4826 - precision: 0.8619 - recall: 0.8137 - f1: 0.8370 - acc: 0.8365 - val_loss: 0.8554 - val_precision: 0.7685 - val_recall: 0.7448 - val_f1: 0.7564 - val_acc: 0.7500\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.59057\n",
            "Epoch 65/100\n",
            " - 41s - loss: 0.4660 - precision: 0.8494 - recall: 0.7800 - f1: 0.8131 - acc: 0.8185 - val_loss: 0.7419 - val_precision: 0.7745 - val_recall: 0.7031 - val_f1: 0.7365 - val_acc: 0.7448\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.59057\n",
            "Epoch 66/100\n",
            " - 43s - loss: 0.5046 - precision: 0.8374 - recall: 0.7837 - f1: 0.8093 - acc: 0.8101 - val_loss: 0.6875 - val_precision: 0.8229 - val_recall: 0.7031 - val_f1: 0.7583 - val_acc: 0.7552\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.59057\n",
            "Epoch 67/100\n",
            " - 42s - loss: 0.4816 - precision: 0.8728 - recall: 0.7969 - f1: 0.8328 - acc: 0.8317 - val_loss: 1.1552 - val_precision: 0.5840 - val_recall: 0.4896 - val_f1: 0.5324 - val_acc: 0.5417\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.59057\n",
            "Epoch 68/100\n",
            " - 42s - loss: 0.5161 - precision: 0.8400 - recall: 0.7764 - f1: 0.8066 - acc: 0.8149 - val_loss: 1.2326 - val_precision: 0.4993 - val_recall: 0.3438 - val_f1: 0.4053 - val_acc: 0.4479\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.59057\n",
            "Epoch 69/100\n",
            " - 42s - loss: 0.4938 - precision: 0.8383 - recall: 0.7909 - f1: 0.8138 - acc: 0.8173 - val_loss: 0.9241 - val_precision: 0.6319 - val_recall: 0.5208 - val_f1: 0.5709 - val_acc: 0.6146\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.59057\n",
            "Epoch 70/100\n",
            " - 41s - loss: 0.5132 - precision: 0.8380 - recall: 0.7849 - f1: 0.8104 - acc: 0.8161 - val_loss: 0.9085 - val_precision: 0.6469 - val_recall: 0.5260 - val_f1: 0.5800 - val_acc: 0.5990\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.59057\n",
            "Epoch 71/100\n",
            " - 41s - loss: 0.4656 - precision: 0.8460 - recall: 0.7981 - f1: 0.8212 - acc: 0.8221 - val_loss: 0.8514 - val_precision: 0.7912 - val_recall: 0.6823 - val_f1: 0.7325 - val_acc: 0.7292\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.59057\n",
            "Epoch 72/100\n",
            " - 41s - loss: 0.4865 - precision: 0.8391 - recall: 0.7861 - f1: 0.8116 - acc: 0.8185 - val_loss: 0.9865 - val_precision: 0.6863 - val_recall: 0.5573 - val_f1: 0.6150 - val_acc: 0.6302\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.59057\n",
            "Epoch 73/100\n",
            " - 42s - loss: 0.4586 - precision: 0.8500 - recall: 0.7849 - f1: 0.8160 - acc: 0.8221 - val_loss: 1.2181 - val_precision: 0.5999 - val_recall: 0.5833 - val_f1: 0.5914 - val_acc: 0.5885\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.59057\n",
            "Epoch 74/100\n",
            " - 42s - loss: 0.4812 - precision: 0.8438 - recall: 0.7921 - f1: 0.8170 - acc: 0.8161 - val_loss: 1.0714 - val_precision: 0.7104 - val_recall: 0.5781 - val_f1: 0.6361 - val_acc: 0.6719\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.59057\n",
            "Epoch 75/100\n",
            " - 41s - loss: 0.4752 - precision: 0.8598 - recall: 0.7849 - f1: 0.8204 - acc: 0.8221 - val_loss: 0.7073 - val_precision: 0.7921 - val_recall: 0.6979 - val_f1: 0.7420 - val_acc: 0.7812\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.59057\n",
            "Epoch 76/100\n",
            " - 42s - loss: 0.4699 - precision: 0.8422 - recall: 0.7873 - f1: 0.8136 - acc: 0.8185 - val_loss: 1.1613 - val_precision: 0.6094 - val_recall: 0.4740 - val_f1: 0.5330 - val_acc: 0.5417\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.59057\n",
            "Epoch 77/100\n",
            " - 42s - loss: 0.4180 - precision: 0.8784 - recall: 0.8233 - f1: 0.8497 - acc: 0.8558 - val_loss: 0.6339 - val_precision: 0.8160 - val_recall: 0.7396 - val_f1: 0.7757 - val_acc: 0.7708\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.59057\n",
            "Epoch 78/100\n",
            " - 42s - loss: 0.3857 - precision: 0.8989 - recall: 0.8353 - f1: 0.8657 - acc: 0.8750 - val_loss: 0.7723 - val_precision: 0.7803 - val_recall: 0.7240 - val_f1: 0.7510 - val_acc: 0.7344\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.59057\n",
            "Epoch 79/100\n",
            " - 42s - loss: 0.3467 - precision: 0.9031 - recall: 0.8510 - f1: 0.8761 - acc: 0.8774 - val_loss: 0.5011 - val_precision: 0.8737 - val_recall: 0.7865 - val_f1: 0.8277 - val_acc: 0.7917\n",
            "\n",
            "Epoch 00079: val_loss improved from 0.59057 to 0.50111, saving model to BREASTNET_FOLD_0.h5\n",
            "Epoch 80/100\n",
            " - 43s - loss: 0.3363 - precision: 0.9065 - recall: 0.8606 - f1: 0.8828 - acc: 0.8810 - val_loss: 0.6435 - val_precision: 0.7986 - val_recall: 0.7448 - val_f1: 0.7707 - val_acc: 0.7708\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.50111\n",
            "Epoch 81/100\n",
            " - 43s - loss: 0.3780 - precision: 0.8999 - recall: 0.8486 - f1: 0.8732 - acc: 0.8714 - val_loss: 0.7065 - val_precision: 0.8082 - val_recall: 0.7292 - val_f1: 0.7665 - val_acc: 0.7500\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.50111\n",
            "Epoch 82/100\n",
            " - 42s - loss: 0.3525 - precision: 0.9035 - recall: 0.8546 - f1: 0.8781 - acc: 0.8714 - val_loss: 0.5522 - val_precision: 0.8195 - val_recall: 0.7344 - val_f1: 0.7740 - val_acc: 0.7708\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.50111\n",
            "Epoch 83/100\n",
            " - 44s - loss: 0.3856 - precision: 0.8949 - recall: 0.8582 - f1: 0.8761 - acc: 0.8798 - val_loss: 0.5463 - val_precision: 0.8351 - val_recall: 0.7917 - val_f1: 0.8128 - val_acc: 0.8073\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.50111\n",
            "Epoch 84/100\n",
            " - 43s - loss: 0.3799 - precision: 0.8816 - recall: 0.8305 - f1: 0.8551 - acc: 0.8546 - val_loss: 0.5478 - val_precision: 0.8284 - val_recall: 0.7552 - val_f1: 0.7899 - val_acc: 0.7865\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.50111\n",
            "Epoch 85/100\n",
            " - 42s - loss: 0.3629 - precision: 0.8988 - recall: 0.8534 - f1: 0.8753 - acc: 0.8762 - val_loss: 0.4540 - val_precision: 0.8920 - val_recall: 0.8125 - val_f1: 0.8498 - val_acc: 0.8281\n",
            "\n",
            "Epoch 00085: val_loss improved from 0.50111 to 0.45404, saving model to BREASTNET_FOLD_0.h5\n",
            "Epoch 86/100\n",
            " - 41s - loss: 0.3937 - precision: 0.8758 - recall: 0.8305 - f1: 0.8525 - acc: 0.8570 - val_loss: 0.5158 - val_precision: 0.8519 - val_recall: 0.7812 - val_f1: 0.8150 - val_acc: 0.8438\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.45404\n",
            "Epoch 87/100\n",
            " - 42s - loss: 0.3699 - precision: 0.8926 - recall: 0.8450 - f1: 0.8680 - acc: 0.8702 - val_loss: 0.5595 - val_precision: 0.8352 - val_recall: 0.7656 - val_f1: 0.7989 - val_acc: 0.8021\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.45404\n",
            "Epoch 88/100\n",
            " - 43s - loss: 0.3765 - precision: 0.8864 - recall: 0.8401 - f1: 0.8625 - acc: 0.8618 - val_loss: 0.9238 - val_precision: 0.7237 - val_recall: 0.6562 - val_f1: 0.6883 - val_acc: 0.6823\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.45404\n",
            "Epoch 89/100\n",
            " - 42s - loss: 0.3369 - precision: 0.9181 - recall: 0.8726 - f1: 0.8947 - acc: 0.9002 - val_loss: 0.7435 - val_precision: 0.8362 - val_recall: 0.7448 - val_f1: 0.7876 - val_acc: 0.7917\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.45404\n",
            "Epoch 90/100\n",
            " - 42s - loss: 0.3724 - precision: 0.8855 - recall: 0.8365 - f1: 0.8601 - acc: 0.8642 - val_loss: 1.4876 - val_precision: 0.5610 - val_recall: 0.5208 - val_f1: 0.5400 - val_acc: 0.5469\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.45404\n",
            "Epoch 91/100\n",
            " - 43s - loss: 0.3534 - precision: 0.8913 - recall: 0.8498 - f1: 0.8700 - acc: 0.8726 - val_loss: 0.6831 - val_precision: 0.7821 - val_recall: 0.7292 - val_f1: 0.7546 - val_acc: 0.7396\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.45404\n",
            "Epoch 92/100\n",
            " - 42s - loss: 0.3228 - precision: 0.8989 - recall: 0.8630 - f1: 0.8804 - acc: 0.8870 - val_loss: 1.0177 - val_precision: 0.6711 - val_recall: 0.6198 - val_f1: 0.6443 - val_acc: 0.6510\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.45404\n",
            "Epoch 93/100\n",
            " - 43s - loss: 0.4272 - precision: 0.8659 - recall: 0.8389 - f1: 0.8521 - acc: 0.8534 - val_loss: 1.7482 - val_precision: 0.3856 - val_recall: 0.3490 - val_f1: 0.3663 - val_acc: 0.3698\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.45404\n",
            "Epoch 94/100\n",
            " - 42s - loss: 0.4485 - precision: 0.8582 - recall: 0.7981 - f1: 0.8267 - acc: 0.8257 - val_loss: 1.2106 - val_precision: 0.7113 - val_recall: 0.6823 - val_f1: 0.6964 - val_acc: 0.7083\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.45404\n",
            "Epoch 95/100\n",
            " - 42s - loss: 0.3815 - precision: 0.8786 - recall: 0.8317 - f1: 0.8542 - acc: 0.8534 - val_loss: 1.4995 - val_precision: 0.6735 - val_recall: 0.6562 - val_f1: 0.6648 - val_acc: 0.6771\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.45404\n",
            "Epoch 96/100\n",
            " - 42s - loss: 0.4043 - precision: 0.8675 - recall: 0.8089 - f1: 0.8370 - acc: 0.8438 - val_loss: 1.1201 - val_precision: 0.7398 - val_recall: 0.7240 - val_f1: 0.7318 - val_acc: 0.7292\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.45404\n",
            "Epoch 97/100\n",
            " - 42s - loss: 0.3793 - precision: 0.8812 - recall: 0.8438 - f1: 0.8620 - acc: 0.8654 - val_loss: 1.2230 - val_precision: 0.6097 - val_recall: 0.4896 - val_f1: 0.5429 - val_acc: 0.5990\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.45404\n",
            "Epoch 98/100\n",
            " - 41s - loss: 0.3705 - precision: 0.8769 - recall: 0.8317 - f1: 0.8535 - acc: 0.8618 - val_loss: 0.8845 - val_precision: 0.7228 - val_recall: 0.6250 - val_f1: 0.6702 - val_acc: 0.6875\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.45404\n",
            "Epoch 99/100\n",
            " - 41s - loss: 0.4584 - precision: 0.8669 - recall: 0.8173 - f1: 0.8412 - acc: 0.8462 - val_loss: 1.3059 - val_precision: 0.6433 - val_recall: 0.5885 - val_f1: 0.6145 - val_acc: 0.6250\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.45404\n",
            "Epoch 100/100\n",
            " - 42s - loss: 0.3823 - precision: 0.8909 - recall: 0.8450 - f1: 0.8673 - acc: 0.8738 - val_loss: 0.7744 - val_precision: 0.7916 - val_recall: 0.7292 - val_f1: 0.7589 - val_acc: 0.7656\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.45404\n",
            "Epoch 1/100\n",
            " - 58s - loss: 0.6407 - precision: 0.7845 - recall: 0.7332 - f1: 0.7578 - acc: 0.7596 - val_loss: 1.0457 - val_precision: 0.5769 - val_recall: 0.4688 - val_f1: 0.5172 - val_acc: 0.5000\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.04573, saving model to BREASTNET_FOLD_1.h5\n",
            "Epoch 2/100\n",
            " - 41s - loss: 0.5545 - precision: 0.8471 - recall: 0.7536 - f1: 0.7972 - acc: 0.8005 - val_loss: 0.8591 - val_precision: 0.7208 - val_recall: 0.6562 - val_f1: 0.6868 - val_acc: 0.6771\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.04573 to 0.85908, saving model to BREASTNET_FOLD_1.h5\n",
            "Epoch 3/100\n",
            " - 43s - loss: 0.5424 - precision: 0.8252 - recall: 0.7572 - f1: 0.7895 - acc: 0.7957 - val_loss: 0.9857 - val_precision: 0.7410 - val_recall: 0.6979 - val_f1: 0.7186 - val_acc: 0.7135\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.85908\n",
            "Epoch 4/100\n",
            " - 42s - loss: 0.5830 - precision: 0.8065 - recall: 0.7464 - f1: 0.7751 - acc: 0.7897 - val_loss: 1.1022 - val_precision: 0.6116 - val_recall: 0.5417 - val_f1: 0.5743 - val_acc: 0.6354\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.85908\n",
            "Epoch 5/100\n",
            " - 42s - loss: 0.5485 - precision: 0.8317 - recall: 0.7849 - f1: 0.8074 - acc: 0.8029 - val_loss: 1.3250 - val_precision: 0.6177 - val_recall: 0.5885 - val_f1: 0.6027 - val_acc: 0.5938\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.85908\n",
            "Epoch 6/100\n",
            " - 42s - loss: 0.5158 - precision: 0.8489 - recall: 0.7897 - f1: 0.8180 - acc: 0.8185 - val_loss: 0.8090 - val_precision: 0.7449 - val_recall: 0.7188 - val_f1: 0.7314 - val_acc: 0.7292\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.85908 to 0.80904, saving model to BREASTNET_FOLD_1.h5\n",
            "Epoch 7/100\n",
            " - 41s - loss: 0.5365 - precision: 0.8428 - recall: 0.7837 - f1: 0.8120 - acc: 0.8185 - val_loss: 2.6734 - val_precision: 0.3829 - val_recall: 0.3490 - val_f1: 0.3650 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.80904\n",
            "Epoch 8/100\n",
            " - 42s - loss: 0.4749 - precision: 0.8496 - recall: 0.8017 - f1: 0.8247 - acc: 0.8245 - val_loss: 1.1883 - val_precision: 0.6423 - val_recall: 0.4844 - val_f1: 0.5521 - val_acc: 0.5938\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.80904\n",
            "Epoch 9/100\n",
            " - 42s - loss: 0.4746 - precision: 0.8553 - recall: 0.7921 - f1: 0.8223 - acc: 0.8281 - val_loss: 1.1500 - val_precision: 0.5353 - val_recall: 0.5104 - val_f1: 0.5225 - val_acc: 0.5365\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.80904\n",
            "Epoch 10/100\n",
            " - 42s - loss: 0.5120 - precision: 0.8445 - recall: 0.7764 - f1: 0.8088 - acc: 0.8113 - val_loss: 1.0664 - val_precision: 0.5714 - val_recall: 0.4375 - val_f1: 0.4956 - val_acc: 0.5781\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.80904\n",
            "Epoch 11/100\n",
            " - 42s - loss: 0.4889 - precision: 0.8399 - recall: 0.7764 - f1: 0.8067 - acc: 0.8089 - val_loss: 2.0140 - val_precision: 0.2166 - val_recall: 0.1719 - val_f1: 0.1917 - val_acc: 0.2656\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.80904\n",
            "Epoch 12/100\n",
            " - 42s - loss: 0.5080 - precision: 0.8266 - recall: 0.7704 - f1: 0.7973 - acc: 0.8101 - val_loss: 2.7477 - val_precision: 0.3288 - val_recall: 0.3125 - val_f1: 0.3204 - val_acc: 0.3542\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.80904\n",
            "Epoch 13/100\n",
            " - 41s - loss: 0.4839 - precision: 0.8391 - recall: 0.7897 - f1: 0.8135 - acc: 0.8161 - val_loss: 1.5758 - val_precision: 0.4374 - val_recall: 0.3906 - val_f1: 0.4126 - val_acc: 0.4167\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.80904\n",
            "Epoch 14/100\n",
            " - 41s - loss: 0.4574 - precision: 0.8601 - recall: 0.7800 - f1: 0.8177 - acc: 0.8245 - val_loss: 1.3983 - val_precision: 0.5272 - val_recall: 0.4427 - val_f1: 0.4809 - val_acc: 0.4948\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.80904\n",
            "Epoch 15/100\n",
            " - 42s - loss: 0.4739 - precision: 0.8491 - recall: 0.7704 - f1: 0.8073 - acc: 0.8089 - val_loss: 0.8115 - val_precision: 0.6858 - val_recall: 0.5104 - val_f1: 0.5844 - val_acc: 0.6198\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.80904\n",
            "Epoch 16/100\n",
            " - 42s - loss: 0.4366 - precision: 0.8673 - recall: 0.7981 - f1: 0.8310 - acc: 0.8341 - val_loss: 1.0703 - val_precision: 0.5538 - val_recall: 0.3750 - val_f1: 0.4466 - val_acc: 0.5833\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.80904\n",
            "Epoch 17/100\n",
            " - 42s - loss: 0.4381 - precision: 0.8774 - recall: 0.8269 - f1: 0.8512 - acc: 0.8510 - val_loss: 0.6940 - val_precision: 0.8213 - val_recall: 0.6458 - val_f1: 0.7229 - val_acc: 0.7500\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.80904 to 0.69403, saving model to BREASTNET_FOLD_1.h5\n",
            "Epoch 18/100\n",
            " - 41s - loss: 0.4507 - precision: 0.8639 - recall: 0.8077 - f1: 0.8346 - acc: 0.8365 - val_loss: 0.8635 - val_precision: 0.7604 - val_recall: 0.7135 - val_f1: 0.7362 - val_acc: 0.7344\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.69403\n",
            "Epoch 19/100\n",
            " - 42s - loss: 0.3831 - precision: 0.8892 - recall: 0.8293 - f1: 0.8580 - acc: 0.8582 - val_loss: 0.6301 - val_precision: 0.8053 - val_recall: 0.7083 - val_f1: 0.7537 - val_acc: 0.7708\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.69403 to 0.63013, saving model to BREASTNET_FOLD_1.h5\n",
            "Epoch 20/100\n",
            " - 42s - loss: 0.3535 - precision: 0.8915 - recall: 0.8425 - f1: 0.8661 - acc: 0.8642 - val_loss: 0.6674 - val_precision: 0.7986 - val_recall: 0.7604 - val_f1: 0.7789 - val_acc: 0.7917\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.63013\n",
            "Epoch 21/100\n",
            " - 43s - loss: 0.4077 - precision: 0.8724 - recall: 0.8209 - f1: 0.8456 - acc: 0.8510 - val_loss: 0.6859 - val_precision: 0.8207 - val_recall: 0.7656 - val_f1: 0.7921 - val_acc: 0.7760\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.63013\n",
            "Epoch 22/100\n",
            " - 44s - loss: 0.3859 - precision: 0.8635 - recall: 0.8197 - f1: 0.8409 - acc: 0.8474 - val_loss: 0.6058 - val_precision: 0.8303 - val_recall: 0.7240 - val_f1: 0.7731 - val_acc: 0.7812\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.63013 to 0.60578, saving model to BREASTNET_FOLD_1.h5\n",
            "Epoch 23/100\n",
            " - 43s - loss: 0.4352 - precision: 0.8618 - recall: 0.8017 - f1: 0.8304 - acc: 0.8269 - val_loss: 0.9514 - val_precision: 0.7594 - val_recall: 0.7396 - val_f1: 0.7492 - val_acc: 0.7552\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.60578\n",
            "Epoch 24/100\n",
            " - 43s - loss: 0.4386 - precision: 0.8568 - recall: 0.8041 - f1: 0.8295 - acc: 0.8317 - val_loss: 0.9589 - val_precision: 0.6379 - val_recall: 0.5885 - val_f1: 0.6121 - val_acc: 0.5990\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.60578\n",
            "Epoch 25/100\n",
            " - 42s - loss: 0.3540 - precision: 0.8867 - recall: 0.8474 - f1: 0.8664 - acc: 0.8642 - val_loss: 0.9782 - val_precision: 0.7134 - val_recall: 0.6615 - val_f1: 0.6864 - val_acc: 0.6979\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.60578\n",
            "Epoch 26/100\n",
            " - 42s - loss: 0.3810 - precision: 0.8880 - recall: 0.8462 - f1: 0.8664 - acc: 0.8642 - val_loss: 0.6840 - val_precision: 0.7448 - val_recall: 0.7135 - val_f1: 0.7288 - val_acc: 0.7344\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.60578\n",
            "Epoch 27/100\n",
            " - 42s - loss: 0.3573 - precision: 0.8895 - recall: 0.8498 - f1: 0.8690 - acc: 0.8822 - val_loss: 0.6064 - val_precision: 0.8210 - val_recall: 0.7604 - val_f1: 0.7895 - val_acc: 0.7865\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.60578\n",
            "Epoch 28/100\n",
            " - 41s - loss: 0.3221 - precision: 0.9098 - recall: 0.8738 - f1: 0.8914 - acc: 0.8918 - val_loss: 0.7380 - val_precision: 0.7983 - val_recall: 0.7604 - val_f1: 0.7786 - val_acc: 0.7969\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.60578\n",
            "Epoch 29/100\n",
            " - 41s - loss: 0.4531 - precision: 0.8550 - recall: 0.8065 - f1: 0.8299 - acc: 0.8377 - val_loss: 1.2582 - val_precision: 0.5556 - val_recall: 0.4948 - val_f1: 0.5234 - val_acc: 0.5312\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.60578\n",
            "Epoch 30/100\n",
            " - 42s - loss: 0.3452 - precision: 0.8899 - recall: 0.8413 - f1: 0.8648 - acc: 0.8630 - val_loss: 0.7385 - val_precision: 0.7799 - val_recall: 0.6979 - val_f1: 0.7365 - val_acc: 0.7292\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.60578\n",
            "Epoch 31/100\n",
            " - 42s - loss: 0.3840 - precision: 0.8949 - recall: 0.8498 - f1: 0.8717 - acc: 0.8678 - val_loss: 0.7945 - val_precision: 0.8028 - val_recall: 0.7188 - val_f1: 0.7584 - val_acc: 0.7604\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.60578\n",
            "Epoch 32/100\n",
            " - 41s - loss: 0.4027 - precision: 0.8738 - recall: 0.8329 - f1: 0.8526 - acc: 0.8510 - val_loss: 0.9213 - val_precision: 0.8072 - val_recall: 0.7448 - val_f1: 0.7745 - val_acc: 0.7760\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.60578\n",
            "Epoch 33/100\n",
            " - 42s - loss: 0.3494 - precision: 0.9007 - recall: 0.8474 - f1: 0.8730 - acc: 0.8810 - val_loss: 0.9828 - val_precision: 0.7551 - val_recall: 0.7083 - val_f1: 0.7310 - val_acc: 0.7240\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.60578\n",
            "Epoch 34/100\n",
            " - 41s - loss: 0.3554 - precision: 0.8905 - recall: 0.8486 - f1: 0.8690 - acc: 0.8762 - val_loss: 0.5666 - val_precision: 0.8463 - val_recall: 0.8021 - val_f1: 0.8235 - val_acc: 0.8177\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.60578 to 0.56657, saving model to BREASTNET_FOLD_1.h5\n",
            "Epoch 35/100\n",
            " - 41s - loss: 0.2975 - precision: 0.9185 - recall: 0.8810 - f1: 0.8993 - acc: 0.8954 - val_loss: 0.4764 - val_precision: 0.8556 - val_recall: 0.7969 - val_f1: 0.8250 - val_acc: 0.8333\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.56657 to 0.47644, saving model to BREASTNET_FOLD_1.h5\n",
            "Epoch 36/100\n",
            " - 40s - loss: 0.2855 - precision: 0.9151 - recall: 0.8846 - f1: 0.8995 - acc: 0.9002 - val_loss: 0.5904 - val_precision: 0.8304 - val_recall: 0.7865 - val_f1: 0.8077 - val_acc: 0.8177\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.47644\n",
            "Epoch 37/100\n",
            " - 41s - loss: 0.2978 - precision: 0.9122 - recall: 0.8858 - f1: 0.8987 - acc: 0.9002 - val_loss: 0.5337 - val_precision: 0.8830 - val_recall: 0.8177 - val_f1: 0.8490 - val_acc: 0.8646\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.47644\n",
            "Epoch 38/100\n",
            " - 42s - loss: 0.2753 - precision: 0.9302 - recall: 0.8930 - f1: 0.9110 - acc: 0.9087 - val_loss: 0.4709 - val_precision: 0.8940 - val_recall: 0.8438 - val_f1: 0.8679 - val_acc: 0.8542\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.47644 to 0.47092, saving model to BREASTNET_FOLD_1.h5\n",
            "Epoch 39/100\n",
            " - 42s - loss: 0.2679 - precision: 0.9242 - recall: 0.8942 - f1: 0.9089 - acc: 0.9135 - val_loss: 0.6193 - val_precision: 0.8021 - val_recall: 0.7604 - val_f1: 0.7807 - val_acc: 0.7917\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.47092\n",
            "Epoch 40/100\n",
            " - 41s - loss: 0.2584 - precision: 0.9220 - recall: 0.8942 - f1: 0.9078 - acc: 0.9099 - val_loss: 0.7977 - val_precision: 0.8029 - val_recall: 0.7656 - val_f1: 0.7836 - val_acc: 0.7760\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.47092\n",
            "Epoch 41/100\n",
            " - 41s - loss: 0.3538 - precision: 0.8906 - recall: 0.8341 - f1: 0.8612 - acc: 0.8522 - val_loss: 0.8693 - val_precision: 0.7083 - val_recall: 0.6146 - val_f1: 0.6580 - val_acc: 0.6667\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.47092\n",
            "Epoch 42/100\n",
            " - 41s - loss: 0.3299 - precision: 0.9035 - recall: 0.8582 - f1: 0.8799 - acc: 0.8786 - val_loss: 0.8218 - val_precision: 0.7884 - val_recall: 0.7604 - val_f1: 0.7741 - val_acc: 0.7865\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.47092\n",
            "Epoch 43/100\n",
            " - 41s - loss: 0.3913 - precision: 0.8827 - recall: 0.8413 - f1: 0.8614 - acc: 0.8666 - val_loss: 0.7262 - val_precision: 0.7679 - val_recall: 0.7240 - val_f1: 0.7452 - val_acc: 0.7448\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.47092\n",
            "Epoch 44/100\n",
            " - 41s - loss: 0.4553 - precision: 0.8507 - recall: 0.8173 - f1: 0.8336 - acc: 0.8305 - val_loss: 2.2623 - val_precision: 0.3992 - val_recall: 0.3750 - val_f1: 0.3867 - val_acc: 0.3854\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.47092\n",
            "Epoch 45/100\n",
            " - 41s - loss: 0.3843 - precision: 0.8973 - recall: 0.8486 - f1: 0.8721 - acc: 0.8750 - val_loss: 0.8349 - val_precision: 0.7607 - val_recall: 0.6458 - val_f1: 0.6986 - val_acc: 0.7396\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.47092\n",
            "Epoch 46/100\n",
            " - 42s - loss: 0.3084 - precision: 0.9131 - recall: 0.8690 - f1: 0.8904 - acc: 0.8870 - val_loss: 0.9137 - val_precision: 0.7176 - val_recall: 0.6875 - val_f1: 0.7022 - val_acc: 0.7135\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.47092\n",
            "Epoch 47/100\n",
            " - 41s - loss: 0.3333 - precision: 0.8963 - recall: 0.8726 - f1: 0.8843 - acc: 0.8822 - val_loss: 0.8469 - val_precision: 0.7250 - val_recall: 0.6719 - val_f1: 0.6973 - val_acc: 0.7083\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.47092\n",
            "Epoch 48/100\n",
            " - 41s - loss: 0.2827 - precision: 0.9153 - recall: 0.8846 - f1: 0.8996 - acc: 0.8966 - val_loss: 0.8502 - val_precision: 0.7448 - val_recall: 0.7135 - val_f1: 0.7288 - val_acc: 0.7396\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.47092\n",
            "Epoch 49/100\n",
            " - 42s - loss: 0.2930 - precision: 0.9255 - recall: 0.8822 - f1: 0.9032 - acc: 0.8990 - val_loss: 0.6581 - val_precision: 0.8113 - val_recall: 0.7812 - val_f1: 0.7960 - val_acc: 0.7917\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.47092\n",
            "Epoch 50/100\n",
            " - 41s - loss: 0.2726 - precision: 0.9189 - recall: 0.8858 - f1: 0.9020 - acc: 0.9038 - val_loss: 0.4450 - val_precision: 0.8646 - val_recall: 0.8646 - val_f1: 0.8646 - val_acc: 0.8646\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.47092 to 0.44503, saving model to BREASTNET_FOLD_1.h5\n",
            "Epoch 51/100\n",
            " - 41s - loss: 0.2490 - precision: 0.9264 - recall: 0.8930 - f1: 0.9093 - acc: 0.9123 - val_loss: 0.4810 - val_precision: 0.8885 - val_recall: 0.8698 - val_f1: 0.8789 - val_acc: 0.8802\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.44503\n",
            "Epoch 52/100\n",
            " - 40s - loss: 0.3036 - precision: 0.9116 - recall: 0.8786 - f1: 0.8947 - acc: 0.8954 - val_loss: 0.5254 - val_precision: 0.8692 - val_recall: 0.7969 - val_f1: 0.8314 - val_acc: 0.8229\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.44503\n",
            "Epoch 53/100\n",
            " - 41s - loss: 0.3335 - precision: 0.9074 - recall: 0.8726 - f1: 0.8895 - acc: 0.8918 - val_loss: 1.0179 - val_precision: 0.6667 - val_recall: 0.5625 - val_f1: 0.6102 - val_acc: 0.6458\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.44503\n",
            "Epoch 54/100\n",
            " - 42s - loss: 0.2668 - precision: 0.9138 - recall: 0.8918 - f1: 0.9026 - acc: 0.9075 - val_loss: 0.8498 - val_precision: 0.7968 - val_recall: 0.7604 - val_f1: 0.7780 - val_acc: 0.7708\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.44503\n",
            "Epoch 55/100\n",
            " - 41s - loss: 0.3597 - precision: 0.8877 - recall: 0.8642 - f1: 0.8757 - acc: 0.8810 - val_loss: 0.5968 - val_precision: 0.8156 - val_recall: 0.8073 - val_f1: 0.8114 - val_acc: 0.8073\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.44503\n",
            "Epoch 56/100\n",
            " - 41s - loss: 0.3706 - precision: 0.8760 - recall: 0.8486 - f1: 0.8620 - acc: 0.8618 - val_loss: 1.6576 - val_precision: 0.3762 - val_recall: 0.3490 - val_f1: 0.3621 - val_acc: 0.3594\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.44503\n",
            "Epoch 57/100\n",
            " - 41s - loss: 0.3754 - precision: 0.8834 - recall: 0.8389 - f1: 0.8604 - acc: 0.8594 - val_loss: 2.5214 - val_precision: 0.3333 - val_recall: 0.2969 - val_f1: 0.3140 - val_acc: 0.3281\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.44503\n",
            "Epoch 58/100\n",
            " - 41s - loss: 0.3042 - precision: 0.9062 - recall: 0.8642 - f1: 0.8845 - acc: 0.8918 - val_loss: 2.1414 - val_precision: 0.4521 - val_recall: 0.4427 - val_f1: 0.4473 - val_acc: 0.4427\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.44503\n",
            "Epoch 59/100\n",
            " - 41s - loss: 0.2893 - precision: 0.9130 - recall: 0.8810 - f1: 0.8965 - acc: 0.8978 - val_loss: 0.8475 - val_precision: 0.7489 - val_recall: 0.6198 - val_f1: 0.6781 - val_acc: 0.6927\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.44503\n",
            "Epoch 60/100\n",
            " - 41s - loss: 0.2757 - precision: 0.9166 - recall: 0.8834 - f1: 0.8996 - acc: 0.9050 - val_loss: 0.6247 - val_precision: 0.8037 - val_recall: 0.7240 - val_f1: 0.7615 - val_acc: 0.7865\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.44503\n",
            "Epoch 61/100\n",
            " - 41s - loss: 0.2523 - precision: 0.9330 - recall: 0.9014 - f1: 0.9169 - acc: 0.9171 - val_loss: 0.5685 - val_precision: 0.8401 - val_recall: 0.7604 - val_f1: 0.7981 - val_acc: 0.7969\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.44503\n",
            "Epoch 62/100\n",
            " - 41s - loss: 0.2685 - precision: 0.9097 - recall: 0.8822 - f1: 0.8956 - acc: 0.8954 - val_loss: 0.6302 - val_precision: 0.7747 - val_recall: 0.7344 - val_f1: 0.7540 - val_acc: 0.7760\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.44503\n",
            "Epoch 63/100\n",
            " - 41s - loss: 0.2248 - precision: 0.9381 - recall: 0.9087 - f1: 0.9231 - acc: 0.9207 - val_loss: 0.8731 - val_precision: 0.6964 - val_recall: 0.6458 - val_f1: 0.6702 - val_acc: 0.6667\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.44503\n",
            "Epoch 64/100\n",
            " - 40s - loss: 0.2125 - precision: 0.9348 - recall: 0.9147 - f1: 0.9246 - acc: 0.9231 - val_loss: 0.7285 - val_precision: 0.7515 - val_recall: 0.6771 - val_f1: 0.7123 - val_acc: 0.7083\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.44503\n",
            "Epoch 65/100\n",
            " - 41s - loss: 0.2301 - precision: 0.9398 - recall: 0.9135 - f1: 0.9263 - acc: 0.9303 - val_loss: 0.6007 - val_precision: 0.8021 - val_recall: 0.7188 - val_f1: 0.7580 - val_acc: 0.7604\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.44503\n",
            "Epoch 66/100\n",
            " - 43s - loss: 0.2661 - precision: 0.9150 - recall: 0.8846 - f1: 0.8994 - acc: 0.8978 - val_loss: 0.6483 - val_precision: 0.7683 - val_recall: 0.7448 - val_f1: 0.7562 - val_acc: 0.7604\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.44503\n",
            "Epoch 67/100\n",
            " - 42s - loss: 0.3852 - precision: 0.8690 - recall: 0.8365 - f1: 0.8524 - acc: 0.8534 - val_loss: 1.1193 - val_precision: 0.7123 - val_recall: 0.6302 - val_f1: 0.6685 - val_acc: 0.6927\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.44503\n",
            "Epoch 68/100\n",
            " - 42s - loss: 0.3043 - precision: 0.9078 - recall: 0.8762 - f1: 0.8916 - acc: 0.8870 - val_loss: 0.7672 - val_precision: 0.7632 - val_recall: 0.7396 - val_f1: 0.7511 - val_acc: 0.7656\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.44503\n",
            "Epoch 69/100\n",
            " - 41s - loss: 0.2709 - precision: 0.9176 - recall: 0.8942 - f1: 0.9057 - acc: 0.9075 - val_loss: 1.2612 - val_precision: 0.6149 - val_recall: 0.5469 - val_f1: 0.5785 - val_acc: 0.6042\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.44503\n",
            "Epoch 70/100\n",
            " - 42s - loss: 0.2584 - precision: 0.9275 - recall: 0.8882 - f1: 0.9073 - acc: 0.9111 - val_loss: 1.2076 - val_precision: 0.5485 - val_recall: 0.4740 - val_f1: 0.5085 - val_acc: 0.5729\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.44503\n",
            "Epoch 71/100\n",
            " - 41s - loss: 0.2632 - precision: 0.9198 - recall: 0.8918 - f1: 0.9055 - acc: 0.9111 - val_loss: 0.9518 - val_precision: 0.6744 - val_recall: 0.5781 - val_f1: 0.6219 - val_acc: 0.6406\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.44503\n",
            "Epoch 72/100\n",
            " - 42s - loss: 0.2716 - precision: 0.9214 - recall: 0.8894 - f1: 0.9050 - acc: 0.9050 - val_loss: 0.9959 - val_precision: 0.7203 - val_recall: 0.6875 - val_f1: 0.7034 - val_acc: 0.6927\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.44503\n",
            "Epoch 73/100\n",
            " - 42s - loss: 0.2239 - precision: 0.9331 - recall: 0.9087 - f1: 0.9206 - acc: 0.9255 - val_loss: 0.6891 - val_precision: 0.8056 - val_recall: 0.7552 - val_f1: 0.7796 - val_acc: 0.7917\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.44503\n",
            "Epoch 74/100\n",
            " - 42s - loss: 0.2417 - precision: 0.9264 - recall: 0.9075 - f1: 0.9168 - acc: 0.9171 - val_loss: 0.6419 - val_precision: 0.8456 - val_recall: 0.8281 - val_f1: 0.8368 - val_acc: 0.8438\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.44503\n",
            "Epoch 75/100\n",
            " - 42s - loss: 0.2849 - precision: 0.9035 - recall: 0.8786 - f1: 0.8908 - acc: 0.8918 - val_loss: 0.8261 - val_precision: 0.7631 - val_recall: 0.7240 - val_f1: 0.7429 - val_acc: 0.7656\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.44503\n",
            "Epoch 76/100\n",
            " - 43s - loss: 0.2241 - precision: 0.9328 - recall: 0.9183 - f1: 0.9255 - acc: 0.9243 - val_loss: 0.8061 - val_precision: 0.7754 - val_recall: 0.7396 - val_f1: 0.7570 - val_acc: 0.7500\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.44503\n",
            "Epoch 77/100\n",
            " - 42s - loss: 0.2426 - precision: 0.9315 - recall: 0.9135 - f1: 0.9224 - acc: 0.9243 - val_loss: 0.7726 - val_precision: 0.7775 - val_recall: 0.7500 - val_f1: 0.7634 - val_acc: 0.7656\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.44503\n",
            "Epoch 78/100\n",
            " - 42s - loss: 0.1829 - precision: 0.9473 - recall: 0.9327 - f1: 0.9399 - acc: 0.9399 - val_loss: 0.5185 - val_precision: 0.8532 - val_recall: 0.8125 - val_f1: 0.8322 - val_acc: 0.8281\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.44503\n",
            "Epoch 79/100\n",
            " - 41s - loss: 0.1947 - precision: 0.9413 - recall: 0.9243 - f1: 0.9326 - acc: 0.9351 - val_loss: 0.7192 - val_precision: 0.8063 - val_recall: 0.7396 - val_f1: 0.7713 - val_acc: 0.7552\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.44503\n",
            "Epoch 80/100\n",
            " - 41s - loss: 0.2023 - precision: 0.9401 - recall: 0.9243 - f1: 0.9320 - acc: 0.9315 - val_loss: 0.5105 - val_precision: 0.8812 - val_recall: 0.8490 - val_f1: 0.8648 - val_acc: 0.8594\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.44503\n",
            "Epoch 81/100\n",
            " - 41s - loss: 0.1930 - precision: 0.9448 - recall: 0.9315 - f1: 0.9381 - acc: 0.9399 - val_loss: 0.5678 - val_precision: 0.8510 - val_recall: 0.8333 - val_f1: 0.8421 - val_acc: 0.8490\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.44503\n",
            "Epoch 82/100\n",
            " - 41s - loss: 0.1582 - precision: 0.9546 - recall: 0.9363 - f1: 0.9453 - acc: 0.9507 - val_loss: 0.5558 - val_precision: 0.8516 - val_recall: 0.8333 - val_f1: 0.8423 - val_acc: 0.8438\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.44503\n",
            "Epoch 83/100\n",
            " - 41s - loss: 0.1381 - precision: 0.9707 - recall: 0.9555 - f1: 0.9631 - acc: 0.9639 - val_loss: 0.5610 - val_precision: 0.8600 - val_recall: 0.8281 - val_f1: 0.8437 - val_acc: 0.8438\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.44503\n",
            "Epoch 84/100\n",
            " - 41s - loss: 0.1669 - precision: 0.9520 - recall: 0.9339 - f1: 0.9427 - acc: 0.9459 - val_loss: 0.7419 - val_precision: 0.8665 - val_recall: 0.8438 - val_f1: 0.8549 - val_acc: 0.8698\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.44503\n",
            "Epoch 85/100\n",
            " - 41s - loss: 0.1639 - precision: 0.9597 - recall: 0.9423 - f1: 0.9508 - acc: 0.9495 - val_loss: 0.6046 - val_precision: 0.8342 - val_recall: 0.8125 - val_f1: 0.8230 - val_acc: 0.8281\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.44503\n",
            "Epoch 86/100\n",
            " - 40s - loss: 0.1956 - precision: 0.9440 - recall: 0.9279 - f1: 0.9358 - acc: 0.9363 - val_loss: 0.5642 - val_precision: 0.8487 - val_recall: 0.8125 - val_f1: 0.8301 - val_acc: 0.8281\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.44503\n",
            "Epoch 87/100\n",
            " - 40s - loss: 0.1674 - precision: 0.9500 - recall: 0.9339 - f1: 0.9418 - acc: 0.9423 - val_loss: 0.6682 - val_precision: 0.8341 - val_recall: 0.8125 - val_f1: 0.8231 - val_acc: 0.8125\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.44503\n",
            "Epoch 88/100\n",
            " - 39s - loss: 0.1497 - precision: 0.9534 - recall: 0.9411 - f1: 0.9472 - acc: 0.9483 - val_loss: 0.5834 - val_precision: 0.8516 - val_recall: 0.8385 - val_f1: 0.8450 - val_acc: 0.8438\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.44503\n",
            "Epoch 89/100\n",
            " - 41s - loss: 0.1401 - precision: 0.9587 - recall: 0.9471 - f1: 0.9528 - acc: 0.9507 - val_loss: 0.5152 - val_precision: 0.8637 - val_recall: 0.8594 - val_f1: 0.8615 - val_acc: 0.8594\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.44503\n",
            "Epoch 90/100\n",
            " - 42s - loss: 0.2003 - precision: 0.9280 - recall: 0.9135 - f1: 0.9207 - acc: 0.9195 - val_loss: 0.5544 - val_precision: 0.8633 - val_recall: 0.8542 - val_f1: 0.8587 - val_acc: 0.8646\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.44503\n",
            "Epoch 91/100\n",
            " - 42s - loss: 0.2058 - precision: 0.9411 - recall: 0.9219 - f1: 0.9314 - acc: 0.9303 - val_loss: 0.9045 - val_precision: 0.8307 - val_recall: 0.8177 - val_f1: 0.8241 - val_acc: 0.8281\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.44503\n",
            "Epoch 92/100\n",
            " - 42s - loss: 0.2273 - precision: 0.9278 - recall: 0.9123 - f1: 0.9199 - acc: 0.9219 - val_loss: 0.8411 - val_precision: 0.8038 - val_recall: 0.7917 - val_f1: 0.7976 - val_acc: 0.7917\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.44503\n",
            "Epoch 93/100\n",
            " - 41s - loss: 0.1973 - precision: 0.9318 - recall: 0.9219 - f1: 0.9268 - acc: 0.9255 - val_loss: 0.8463 - val_precision: 0.7665 - val_recall: 0.7500 - val_f1: 0.7581 - val_acc: 0.7656\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.44503\n",
            "Epoch 94/100\n",
            " - 40s - loss: 0.2380 - precision: 0.9280 - recall: 0.9014 - f1: 0.9145 - acc: 0.9159 - val_loss: 1.2590 - val_precision: 0.6128 - val_recall: 0.5469 - val_f1: 0.5778 - val_acc: 0.5781\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.44503\n",
            "Epoch 95/100\n",
            " - 41s - loss: 0.2372 - precision: 0.9254 - recall: 0.9087 - f1: 0.9169 - acc: 0.9195 - val_loss: 1.6813 - val_precision: 0.6590 - val_recall: 0.6562 - val_f1: 0.6576 - val_acc: 0.6615\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.44503\n",
            "Epoch 96/100\n",
            " - 41s - loss: 0.2578 - precision: 0.8992 - recall: 0.8786 - f1: 0.8887 - acc: 0.8882 - val_loss: 1.0647 - val_precision: 0.7661 - val_recall: 0.7344 - val_f1: 0.7499 - val_acc: 0.7344\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.44503\n",
            "Epoch 97/100\n",
            " - 40s - loss: 0.2469 - precision: 0.9232 - recall: 0.8990 - f1: 0.9108 - acc: 0.9111 - val_loss: 0.8174 - val_precision: 0.8506 - val_recall: 0.8333 - val_f1: 0.8418 - val_acc: 0.8385\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.44503\n",
            "Epoch 98/100\n",
            " - 41s - loss: 0.2293 - precision: 0.9361 - recall: 0.9123 - f1: 0.9239 - acc: 0.9219 - val_loss: 0.6835 - val_precision: 0.7897 - val_recall: 0.7188 - val_f1: 0.7522 - val_acc: 0.7656\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.44503\n",
            "Epoch 99/100\n",
            " - 41s - loss: 0.2211 - precision: 0.9237 - recall: 0.9026 - f1: 0.9130 - acc: 0.9159 - val_loss: 0.7690 - val_precision: 0.7581 - val_recall: 0.7344 - val_f1: 0.7460 - val_acc: 0.7448\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.44503\n",
            "Epoch 100/100\n",
            " - 40s - loss: 0.2191 - precision: 0.9229 - recall: 0.9075 - f1: 0.9151 - acc: 0.9171 - val_loss: 1.0098 - val_precision: 0.7632 - val_recall: 0.7552 - val_f1: 0.7592 - val_acc: 0.7604\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.44503\n",
            "Epoch 1/100\n",
            " - 55s - loss: 0.4190 - precision: 0.8841 - recall: 0.8341 - f1: 0.8583 - acc: 0.8642 - val_loss: 1.0668 - val_precision: 0.7317 - val_recall: 0.7240 - val_f1: 0.7278 - val_acc: 0.7240\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.06683, saving model to BREASTNET_FOLD_2.h5\n",
            "Epoch 2/100\n",
            " - 37s - loss: 0.3370 - precision: 0.8969 - recall: 0.8642 - f1: 0.8801 - acc: 0.8810 - val_loss: 0.9533 - val_precision: 0.7584 - val_recall: 0.7552 - val_f1: 0.7568 - val_acc: 0.7552\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.06683 to 0.95331, saving model to BREASTNET_FOLD_2.h5\n",
            "Epoch 3/100\n",
            " - 40s - loss: 0.3266 - precision: 0.8965 - recall: 0.8762 - f1: 0.8861 - acc: 0.8846 - val_loss: 0.9891 - val_precision: 0.7364 - val_recall: 0.6979 - val_f1: 0.7166 - val_acc: 0.7240\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.95331\n",
            "Epoch 4/100\n",
            " - 39s - loss: 0.3606 - precision: 0.8947 - recall: 0.8666 - f1: 0.8803 - acc: 0.8810 - val_loss: 1.0073 - val_precision: 0.7722 - val_recall: 0.7604 - val_f1: 0.7662 - val_acc: 0.7604\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.95331\n",
            "Epoch 5/100\n",
            " - 40s - loss: 0.3162 - precision: 0.9084 - recall: 0.8786 - f1: 0.8932 - acc: 0.8978 - val_loss: 1.0552 - val_precision: 0.7808 - val_recall: 0.7604 - val_f1: 0.7705 - val_acc: 0.7812\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.95331\n",
            "Epoch 6/100\n",
            " - 40s - loss: 0.2723 - precision: 0.9166 - recall: 0.8966 - f1: 0.9064 - acc: 0.9002 - val_loss: 0.4981 - val_precision: 0.8138 - val_recall: 0.7917 - val_f1: 0.8024 - val_acc: 0.8021\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.95331 to 0.49811, saving model to BREASTNET_FOLD_2.h5\n",
            "Epoch 7/100\n",
            " - 40s - loss: 0.3423 - precision: 0.8963 - recall: 0.8594 - f1: 0.8773 - acc: 0.8762 - val_loss: 0.6916 - val_precision: 0.7817 - val_recall: 0.7448 - val_f1: 0.7627 - val_acc: 0.7552\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.49811\n",
            "Epoch 8/100\n",
            " - 41s - loss: 0.2733 - precision: 0.9123 - recall: 0.8882 - f1: 0.9000 - acc: 0.9014 - val_loss: 0.9905 - val_precision: 0.7162 - val_recall: 0.6979 - val_f1: 0.7069 - val_acc: 0.6979\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.49811\n",
            "Epoch 9/100\n",
            " - 41s - loss: 0.2463 - precision: 0.9347 - recall: 0.9123 - f1: 0.9232 - acc: 0.9231 - val_loss: 0.4240 - val_precision: 0.8614 - val_recall: 0.8385 - val_f1: 0.8498 - val_acc: 0.8542\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.49811 to 0.42399, saving model to BREASTNET_FOLD_2.h5\n",
            "Epoch 10/100\n",
            " - 42s - loss: 0.2963 - precision: 0.9049 - recall: 0.8798 - f1: 0.8921 - acc: 0.8954 - val_loss: 0.6502 - val_precision: 0.8307 - val_recall: 0.7969 - val_f1: 0.8134 - val_acc: 0.8073\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.42399\n",
            "Epoch 11/100\n",
            " - 43s - loss: 0.3304 - precision: 0.8864 - recall: 0.8606 - f1: 0.8732 - acc: 0.8798 - val_loss: 0.8044 - val_precision: 0.7552 - val_recall: 0.7240 - val_f1: 0.7392 - val_acc: 0.7396\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.42399\n",
            "Epoch 12/100\n",
            " - 43s - loss: 0.2997 - precision: 0.9013 - recall: 0.8774 - f1: 0.8892 - acc: 0.8846 - val_loss: 0.4128 - val_precision: 0.8671 - val_recall: 0.8438 - val_f1: 0.8552 - val_acc: 0.8594\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.42399 to 0.41281, saving model to BREASTNET_FOLD_2.h5\n",
            "Epoch 13/100\n",
            " - 42s - loss: 0.2869 - precision: 0.9135 - recall: 0.8906 - f1: 0.9018 - acc: 0.9050 - val_loss: 0.5733 - val_precision: 0.7893 - val_recall: 0.7812 - val_f1: 0.7852 - val_acc: 0.7865\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.41281\n",
            "Epoch 14/100\n",
            " - 41s - loss: 0.2437 - precision: 0.9277 - recall: 0.9099 - f1: 0.9186 - acc: 0.9183 - val_loss: 0.3559 - val_precision: 0.8767 - val_recall: 0.8490 - val_f1: 0.8625 - val_acc: 0.8646\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.41281 to 0.35590, saving model to BREASTNET_FOLD_2.h5\n",
            "Epoch 15/100\n",
            " - 41s - loss: 0.2371 - precision: 0.9322 - recall: 0.9099 - f1: 0.9208 - acc: 0.9183 - val_loss: 0.3230 - val_precision: 0.8787 - val_recall: 0.8646 - val_f1: 0.8715 - val_acc: 0.8646\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.35590 to 0.32304, saving model to BREASTNET_FOLD_2.h5\n",
            "Epoch 16/100\n",
            " - 41s - loss: 0.2892 - precision: 0.9141 - recall: 0.8822 - f1: 0.8978 - acc: 0.9038 - val_loss: 0.7227 - val_precision: 0.7921 - val_recall: 0.7708 - val_f1: 0.7813 - val_acc: 0.7865\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.32304\n",
            "Epoch 17/100\n",
            " - 42s - loss: 0.3219 - precision: 0.8955 - recall: 0.8738 - f1: 0.8844 - acc: 0.8822 - val_loss: 0.7509 - val_precision: 0.7390 - val_recall: 0.6979 - val_f1: 0.7175 - val_acc: 0.7135\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.32304\n",
            "Epoch 18/100\n",
            " - 42s - loss: 0.2869 - precision: 0.9107 - recall: 0.8954 - f1: 0.9030 - acc: 0.9050 - val_loss: 0.9725 - val_precision: 0.7615 - val_recall: 0.7292 - val_f1: 0.7449 - val_acc: 0.7500\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.32304\n",
            "Epoch 19/100\n",
            " - 42s - loss: 0.2721 - precision: 0.9195 - recall: 0.8966 - f1: 0.9078 - acc: 0.9075 - val_loss: 0.3763 - val_precision: 0.8886 - val_recall: 0.8750 - val_f1: 0.8817 - val_acc: 0.8854\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.32304\n",
            "Epoch 20/100\n",
            " - 41s - loss: 0.2164 - precision: 0.9283 - recall: 0.9075 - f1: 0.9177 - acc: 0.9219 - val_loss: 0.4049 - val_precision: 0.8632 - val_recall: 0.8542 - val_f1: 0.8586 - val_acc: 0.8646\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.32304\n",
            "Epoch 21/100\n",
            " - 40s - loss: 0.2377 - precision: 0.9134 - recall: 0.8894 - f1: 0.9011 - acc: 0.9038 - val_loss: 0.5187 - val_precision: 0.8254 - val_recall: 0.8125 - val_f1: 0.8188 - val_acc: 0.8229\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.32304\n",
            "Epoch 22/100\n",
            " - 41s - loss: 0.2588 - precision: 0.9170 - recall: 0.9014 - f1: 0.9091 - acc: 0.9038 - val_loss: 0.4806 - val_precision: 0.8125 - val_recall: 0.8125 - val_f1: 0.8125 - val_acc: 0.8125\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.32304\n",
            "Epoch 23/100\n",
            " - 41s - loss: 0.2529 - precision: 0.9279 - recall: 0.9123 - f1: 0.9200 - acc: 0.9207 - val_loss: 0.9377 - val_precision: 0.7545 - val_recall: 0.5885 - val_f1: 0.6611 - val_acc: 0.6667\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.32304\n",
            "Epoch 24/100\n",
            " - 41s - loss: 0.2492 - precision: 0.9285 - recall: 0.9062 - f1: 0.9172 - acc: 0.9219 - val_loss: 0.5204 - val_precision: 0.8633 - val_recall: 0.8542 - val_f1: 0.8587 - val_acc: 0.8542\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.32304\n",
            "Epoch 25/100\n",
            " - 40s - loss: 0.2691 - precision: 0.9112 - recall: 0.8786 - f1: 0.8945 - acc: 0.8930 - val_loss: 0.6237 - val_precision: 0.8366 - val_recall: 0.8021 - val_f1: 0.8188 - val_acc: 0.8281\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.32304\n",
            "Epoch 26/100\n",
            " - 41s - loss: 0.2013 - precision: 0.9388 - recall: 0.9183 - f1: 0.9283 - acc: 0.9279 - val_loss: 0.4136 - val_precision: 0.8599 - val_recall: 0.8281 - val_f1: 0.8436 - val_acc: 0.8542\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.32304\n",
            "Epoch 27/100\n",
            " - 41s - loss: 0.2175 - precision: 0.9205 - recall: 0.9062 - f1: 0.9132 - acc: 0.9183 - val_loss: 0.3875 - val_precision: 0.8844 - val_recall: 0.8385 - val_f1: 0.8609 - val_acc: 0.8750\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.32304\n",
            "Epoch 28/100\n",
            " - 41s - loss: 0.2017 - precision: 0.9293 - recall: 0.9171 - f1: 0.9231 - acc: 0.9207 - val_loss: 1.6057 - val_precision: 0.4501 - val_recall: 0.4167 - val_f1: 0.4326 - val_acc: 0.4323\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.32304\n",
            "Epoch 29/100\n",
            " - 41s - loss: 0.2378 - precision: 0.9257 - recall: 0.8978 - f1: 0.9115 - acc: 0.9123 - val_loss: 0.3913 - val_precision: 0.8900 - val_recall: 0.8854 - val_f1: 0.8877 - val_acc: 0.8854\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.32304\n",
            "Epoch 30/100\n",
            " - 40s - loss: 0.2482 - precision: 0.9336 - recall: 0.9123 - f1: 0.9227 - acc: 0.9255 - val_loss: 0.7421 - val_precision: 0.7681 - val_recall: 0.7083 - val_f1: 0.7370 - val_acc: 0.7500\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.32304\n",
            "Epoch 31/100\n",
            " - 41s - loss: 0.2260 - precision: 0.9298 - recall: 0.9099 - f1: 0.9196 - acc: 0.9195 - val_loss: 0.6317 - val_precision: 0.7734 - val_recall: 0.7292 - val_f1: 0.7506 - val_acc: 0.7656\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.32304\n",
            "Epoch 32/100\n",
            " - 41s - loss: 0.2969 - precision: 0.9076 - recall: 0.8858 - f1: 0.8965 - acc: 0.8966 - val_loss: 0.4060 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.32304\n",
            "Epoch 33/100\n",
            " - 40s - loss: 0.2617 - precision: 0.9096 - recall: 0.8954 - f1: 0.9024 - acc: 0.9002 - val_loss: 0.7066 - val_precision: 0.6998 - val_recall: 0.6927 - val_f1: 0.6962 - val_acc: 0.6927\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.32304\n",
            "Epoch 34/100\n",
            " - 41s - loss: 0.2069 - precision: 0.9303 - recall: 0.9147 - f1: 0.9224 - acc: 0.9231 - val_loss: 0.3873 - val_precision: 0.8881 - val_recall: 0.8698 - val_f1: 0.8788 - val_acc: 0.8750\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.32304\n",
            "Epoch 35/100\n",
            " - 40s - loss: 0.2276 - precision: 0.9353 - recall: 0.9183 - f1: 0.9267 - acc: 0.9279 - val_loss: 0.3767 - val_precision: 0.8782 - val_recall: 0.8646 - val_f1: 0.8713 - val_acc: 0.8802\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.32304\n",
            "Epoch 36/100\n",
            " - 41s - loss: 0.1569 - precision: 0.9503 - recall: 0.9387 - f1: 0.9444 - acc: 0.9471 - val_loss: 0.2939 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.32304 to 0.29386, saving model to BREASTNET_FOLD_2.h5\n",
            "Epoch 37/100\n",
            " - 40s - loss: 0.1663 - precision: 0.9431 - recall: 0.9243 - f1: 0.9335 - acc: 0.9363 - val_loss: 0.3642 - val_precision: 0.9137 - val_recall: 0.8854 - val_f1: 0.8993 - val_acc: 0.9062\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.29386\n",
            "Epoch 38/100\n",
            " - 41s - loss: 0.1615 - precision: 0.9546 - recall: 0.9351 - f1: 0.9446 - acc: 0.9435 - val_loss: 0.3310 - val_precision: 0.8977 - val_recall: 0.8698 - val_f1: 0.8835 - val_acc: 0.8906\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.29386\n",
            "Epoch 39/100\n",
            " - 41s - loss: 0.1701 - precision: 0.9524 - recall: 0.9399 - f1: 0.9461 - acc: 0.9459 - val_loss: 0.4272 - val_precision: 0.8628 - val_recall: 0.8490 - val_f1: 0.8557 - val_acc: 0.8542\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.29386\n",
            "Epoch 40/100\n",
            " - 41s - loss: 0.1666 - precision: 0.9545 - recall: 0.9375 - f1: 0.9459 - acc: 0.9483 - val_loss: 0.4705 - val_precision: 0.8550 - val_recall: 0.8281 - val_f1: 0.8412 - val_acc: 0.8385\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.29386\n",
            "Epoch 41/100\n",
            " - 40s - loss: 0.1882 - precision: 0.9323 - recall: 0.9255 - f1: 0.9289 - acc: 0.9279 - val_loss: 0.7029 - val_precision: 0.8229 - val_recall: 0.8229 - val_f1: 0.8229 - val_acc: 0.8229\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.29386\n",
            "Epoch 42/100\n",
            " - 41s - loss: 0.1829 - precision: 0.9418 - recall: 0.9327 - f1: 0.9372 - acc: 0.9375 - val_loss: 0.5225 - val_precision: 0.8581 - val_recall: 0.8490 - val_f1: 0.8535 - val_acc: 0.8594\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.29386\n",
            "Epoch 43/100\n",
            " - 40s - loss: 0.2174 - precision: 0.9330 - recall: 0.9183 - f1: 0.9255 - acc: 0.9303 - val_loss: 0.7066 - val_precision: 0.8125 - val_recall: 0.8125 - val_f1: 0.8125 - val_acc: 0.8125\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.29386\n",
            "Epoch 44/100\n",
            " - 40s - loss: 0.2153 - precision: 0.9325 - recall: 0.9135 - f1: 0.9228 - acc: 0.9159 - val_loss: 1.9328 - val_precision: 0.5079 - val_recall: 0.4792 - val_f1: 0.4930 - val_acc: 0.4896\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.29386\n",
            "Epoch 45/100\n",
            " - 41s - loss: 0.2246 - precision: 0.9374 - recall: 0.9135 - f1: 0.9252 - acc: 0.9231 - val_loss: 2.2727 - val_precision: 0.3744 - val_recall: 0.3542 - val_f1: 0.3639 - val_acc: 0.3698\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.29386\n",
            "Epoch 46/100\n",
            " - 40s - loss: 0.1779 - precision: 0.9383 - recall: 0.9303 - f1: 0.9342 - acc: 0.9351 - val_loss: 0.9519 - val_precision: 0.6898 - val_recall: 0.6250 - val_f1: 0.6557 - val_acc: 0.6562\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.29386\n",
            "Epoch 47/100\n",
            " - 40s - loss: 0.1713 - precision: 0.9497 - recall: 0.9375 - f1: 0.9435 - acc: 0.9459 - val_loss: 0.9304 - val_precision: 0.6592 - val_recall: 0.6042 - val_f1: 0.6303 - val_acc: 0.6302\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.29386\n",
            "Epoch 48/100\n",
            " - 40s - loss: 0.1492 - precision: 0.9514 - recall: 0.9399 - f1: 0.9456 - acc: 0.9459 - val_loss: 0.6162 - val_precision: 0.7679 - val_recall: 0.7188 - val_f1: 0.7422 - val_acc: 0.7604\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.29386\n",
            "Epoch 49/100\n",
            " - 40s - loss: 0.1390 - precision: 0.9625 - recall: 0.9555 - f1: 0.9590 - acc: 0.9567 - val_loss: 0.6130 - val_precision: 0.8236 - val_recall: 0.7917 - val_f1: 0.8071 - val_acc: 0.8125\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.29386\n",
            "Epoch 50/100\n",
            " - 40s - loss: 0.1360 - precision: 0.9562 - recall: 0.9471 - f1: 0.9516 - acc: 0.9567 - val_loss: 0.4331 - val_precision: 0.8735 - val_recall: 0.8281 - val_f1: 0.8501 - val_acc: 0.8594\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.29386\n",
            "Epoch 51/100\n",
            " - 40s - loss: 0.1363 - precision: 0.9552 - recall: 0.9447 - f1: 0.9499 - acc: 0.9507 - val_loss: 0.3807 - val_precision: 0.8646 - val_recall: 0.8646 - val_f1: 0.8646 - val_acc: 0.8646\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.29386\n",
            "Epoch 52/100\n",
            " - 42s - loss: 0.1460 - precision: 0.9587 - recall: 0.9495 - f1: 0.9541 - acc: 0.9567 - val_loss: 0.4985 - val_precision: 0.8534 - val_recall: 0.8490 - val_f1: 0.8512 - val_acc: 0.8490\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.29386\n",
            "Epoch 53/100\n",
            " - 42s - loss: 0.1570 - precision: 0.9457 - recall: 0.9411 - f1: 0.9434 - acc: 0.9423 - val_loss: 0.7222 - val_precision: 0.7940 - val_recall: 0.7865 - val_f1: 0.7902 - val_acc: 0.7865\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.29386\n",
            "Epoch 54/100\n",
            " - 41s - loss: 0.1790 - precision: 0.9377 - recall: 0.9243 - f1: 0.9309 - acc: 0.9327 - val_loss: 0.5971 - val_precision: 0.8438 - val_recall: 0.8438 - val_f1: 0.8437 - val_acc: 0.8438\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.29386\n",
            "Epoch 55/100\n",
            " - 42s - loss: 0.1953 - precision: 0.9439 - recall: 0.9291 - f1: 0.9364 - acc: 0.9387 - val_loss: 0.6263 - val_precision: 0.8594 - val_recall: 0.8594 - val_f1: 0.8594 - val_acc: 0.8594\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.29386\n",
            "Epoch 56/100\n",
            " - 41s - loss: 0.2314 - precision: 0.9220 - recall: 0.9087 - f1: 0.9152 - acc: 0.9147 - val_loss: 0.9186 - val_precision: 0.8065 - val_recall: 0.7812 - val_f1: 0.7936 - val_acc: 0.7969\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.29386\n",
            "Epoch 57/100\n",
            " - 41s - loss: 0.2103 - precision: 0.9318 - recall: 0.9219 - f1: 0.9268 - acc: 0.9243 - val_loss: 0.9013 - val_precision: 0.7771 - val_recall: 0.7604 - val_f1: 0.7686 - val_acc: 0.7760\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.29386\n",
            "Epoch 58/100\n",
            " - 40s - loss: 0.1913 - precision: 0.9244 - recall: 0.9111 - f1: 0.9177 - acc: 0.9171 - val_loss: 1.1386 - val_precision: 0.7579 - val_recall: 0.7500 - val_f1: 0.7539 - val_acc: 0.7500\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.29386\n",
            "Epoch 59/100\n",
            " - 42s - loss: 0.1924 - precision: 0.9392 - recall: 0.9303 - f1: 0.9347 - acc: 0.9363 - val_loss: 0.6568 - val_precision: 0.8418 - val_recall: 0.8333 - val_f1: 0.8375 - val_acc: 0.8333\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.29386\n",
            "Epoch 60/100\n",
            " - 41s - loss: 0.1349 - precision: 0.9598 - recall: 0.9471 - f1: 0.9533 - acc: 0.9519 - val_loss: 0.7053 - val_precision: 0.8177 - val_recall: 0.8177 - val_f1: 0.8177 - val_acc: 0.8177\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.29386\n",
            "Epoch 61/100\n",
            " - 40s - loss: 0.1589 - precision: 0.9490 - recall: 0.9399 - f1: 0.9444 - acc: 0.9447 - val_loss: 0.5134 - val_precision: 0.8594 - val_recall: 0.8594 - val_f1: 0.8594 - val_acc: 0.8594\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.29386\n",
            "Epoch 62/100\n",
            " - 41s - loss: 0.1275 - precision: 0.9612 - recall: 0.9543 - f1: 0.9578 - acc: 0.9591 - val_loss: 0.5599 - val_precision: 0.8742 - val_recall: 0.8698 - val_f1: 0.8720 - val_acc: 0.8698\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.29386\n",
            "Epoch 63/100\n",
            " - 41s - loss: 0.1417 - precision: 0.9637 - recall: 0.9567 - f1: 0.9602 - acc: 0.9615 - val_loss: 0.4760 - val_precision: 0.8743 - val_recall: 0.8698 - val_f1: 0.8720 - val_acc: 0.8698\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.29386\n",
            "Epoch 64/100\n",
            " - 40s - loss: 0.1278 - precision: 0.9648 - recall: 0.9579 - f1: 0.9614 - acc: 0.9591 - val_loss: 0.5989 - val_precision: 0.8212 - val_recall: 0.8125 - val_f1: 0.8168 - val_acc: 0.8229\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.29386\n",
            "Epoch 65/100\n",
            " - 41s - loss: 0.1267 - precision: 0.9635 - recall: 0.9495 - f1: 0.9564 - acc: 0.9579 - val_loss: 0.5198 - val_precision: 0.8542 - val_recall: 0.8542 - val_f1: 0.8542 - val_acc: 0.8542\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.29386\n",
            "Epoch 66/100\n",
            " - 40s - loss: 0.1711 - precision: 0.9503 - recall: 0.9399 - f1: 0.9451 - acc: 0.9447 - val_loss: 1.3089 - val_precision: 0.6314 - val_recall: 0.6146 - val_f1: 0.6229 - val_acc: 0.6198\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.29386\n",
            "Epoch 67/100\n",
            " - 40s - loss: 0.1642 - precision: 0.9388 - recall: 0.9243 - f1: 0.9314 - acc: 0.9351 - val_loss: 0.5395 - val_precision: 0.8732 - val_recall: 0.8229 - val_f1: 0.8473 - val_acc: 0.8646\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.29386\n",
            "Epoch 68/100\n",
            " - 40s - loss: 0.2136 - precision: 0.9270 - recall: 0.9159 - f1: 0.9213 - acc: 0.9243 - val_loss: 1.1931 - val_precision: 0.6633 - val_recall: 0.6562 - val_f1: 0.6597 - val_acc: 0.6562\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.29386\n",
            "Epoch 69/100\n",
            " - 41s - loss: 0.2034 - precision: 0.9279 - recall: 0.9099 - f1: 0.9187 - acc: 0.9207 - val_loss: 0.6238 - val_precision: 0.8068 - val_recall: 0.7604 - val_f1: 0.7829 - val_acc: 0.7865\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.29386\n",
            "Epoch 70/100\n",
            " - 40s - loss: 0.1964 - precision: 0.9334 - recall: 0.9255 - f1: 0.9294 - acc: 0.9267 - val_loss: 1.1362 - val_precision: 0.7135 - val_recall: 0.7135 - val_f1: 0.7135 - val_acc: 0.7135\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.29386\n",
            "Epoch 71/100\n",
            " - 41s - loss: 0.1964 - precision: 0.9362 - recall: 0.9195 - f1: 0.9277 - acc: 0.9291 - val_loss: 1.2757 - val_precision: 0.6024 - val_recall: 0.5938 - val_f1: 0.5980 - val_acc: 0.5938\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.29386\n",
            "Epoch 72/100\n",
            " - 41s - loss: 0.1789 - precision: 0.9312 - recall: 0.9267 - f1: 0.9289 - acc: 0.9303 - val_loss: 1.2059 - val_precision: 0.6552 - val_recall: 0.6354 - val_f1: 0.6451 - val_acc: 0.6458\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.29386\n",
            "Epoch 73/100\n",
            " - 40s - loss: 0.1743 - precision: 0.9381 - recall: 0.9303 - f1: 0.9342 - acc: 0.9339 - val_loss: 0.6030 - val_precision: 0.8107 - val_recall: 0.7812 - val_f1: 0.7956 - val_acc: 0.7917\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.29386\n",
            "Epoch 74/100\n",
            " - 40s - loss: 0.1999 - precision: 0.9342 - recall: 0.9231 - f1: 0.9286 - acc: 0.9267 - val_loss: 1.0769 - val_precision: 0.6569 - val_recall: 0.6094 - val_f1: 0.6321 - val_acc: 0.6458\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.29386\n",
            "Epoch 75/100\n",
            " - 40s - loss: 0.1604 - precision: 0.9463 - recall: 0.9339 - f1: 0.9400 - acc: 0.9399 - val_loss: 0.6398 - val_precision: 0.7948 - val_recall: 0.7708 - val_f1: 0.7825 - val_acc: 0.7917\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.29386\n",
            "Epoch 76/100\n",
            " - 40s - loss: 0.1413 - precision: 0.9621 - recall: 0.9447 - f1: 0.9532 - acc: 0.9543 - val_loss: 0.5484 - val_precision: 0.8085 - val_recall: 0.7917 - val_f1: 0.8000 - val_acc: 0.7917\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.29386\n",
            "Epoch 77/100\n",
            " - 40s - loss: 0.1831 - precision: 0.9430 - recall: 0.9351 - f1: 0.9390 - acc: 0.9399 - val_loss: 0.5913 - val_precision: 0.8042 - val_recall: 0.7917 - val_f1: 0.7978 - val_acc: 0.8021\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.29386\n",
            "Epoch 78/100\n",
            " - 40s - loss: 0.1149 - precision: 0.9625 - recall: 0.9567 - f1: 0.9596 - acc: 0.9591 - val_loss: 0.4415 - val_precision: 0.8923 - val_recall: 0.8594 - val_f1: 0.8755 - val_acc: 0.8698\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.29386\n",
            "Epoch 79/100\n",
            " - 40s - loss: 0.1126 - precision: 0.9673 - recall: 0.9567 - f1: 0.9620 - acc: 0.9603 - val_loss: 0.6809 - val_precision: 0.7542 - val_recall: 0.7188 - val_f1: 0.7360 - val_acc: 0.7500\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.29386\n",
            "Epoch 80/100\n",
            " - 40s - loss: 0.1159 - precision: 0.9638 - recall: 0.9603 - f1: 0.9621 - acc: 0.9627 - val_loss: 0.5557 - val_precision: 0.8307 - val_recall: 0.8177 - val_f1: 0.8241 - val_acc: 0.8229\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.29386\n",
            "Epoch 81/100\n",
            " - 40s - loss: 0.0898 - precision: 0.9710 - recall: 0.9675 - f1: 0.9693 - acc: 0.9688 - val_loss: 0.3909 - val_precision: 0.8677 - val_recall: 0.8542 - val_f1: 0.8609 - val_acc: 0.8594\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.29386\n",
            "Epoch 82/100\n",
            " - 40s - loss: 0.1195 - precision: 0.9637 - recall: 0.9579 - f1: 0.9608 - acc: 0.9639 - val_loss: 0.3457 - val_precision: 0.8990 - val_recall: 0.8802 - val_f1: 0.8895 - val_acc: 0.8906\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.29386\n",
            "Epoch 83/100\n",
            " - 41s - loss: 0.1171 - precision: 0.9661 - recall: 0.9579 - f1: 0.9620 - acc: 0.9603 - val_loss: 0.4131 - val_precision: 0.8623 - val_recall: 0.8490 - val_f1: 0.8556 - val_acc: 0.8542\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.29386\n",
            "Epoch 84/100\n",
            " - 40s - loss: 0.1023 - precision: 0.9710 - recall: 0.9627 - f1: 0.9668 - acc: 0.9675 - val_loss: 0.3021 - val_precision: 0.9042 - val_recall: 0.8802 - val_f1: 0.8920 - val_acc: 0.8906\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.29386\n",
            "Epoch 85/100\n",
            " - 39s - loss: 0.1102 - precision: 0.9708 - recall: 0.9615 - f1: 0.9661 - acc: 0.9675 - val_loss: 0.4333 - val_precision: 0.8471 - val_recall: 0.8385 - val_f1: 0.8428 - val_acc: 0.8438\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.29386\n",
            "Epoch 86/100\n",
            " - 40s - loss: 0.0917 - precision: 0.9686 - recall: 0.9639 - f1: 0.9663 - acc: 0.9663 - val_loss: 0.4508 - val_precision: 0.8846 - val_recall: 0.8750 - val_f1: 0.8798 - val_acc: 0.8750\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.29386\n",
            "Epoch 87/100\n",
            " - 40s - loss: 0.0903 - precision: 0.9758 - recall: 0.9700 - f1: 0.9728 - acc: 0.9724 - val_loss: 0.3010 - val_precision: 0.9160 - val_recall: 0.9115 - val_f1: 0.9137 - val_acc: 0.9167\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.29386\n",
            "Epoch 88/100\n",
            " - 39s - loss: 0.1025 - precision: 0.9708 - recall: 0.9627 - f1: 0.9667 - acc: 0.9663 - val_loss: 0.5086 - val_precision: 0.8677 - val_recall: 0.8542 - val_f1: 0.8609 - val_acc: 0.8542\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.29386\n",
            "Epoch 89/100\n",
            " - 40s - loss: 0.1034 - precision: 0.9722 - recall: 0.9663 - f1: 0.9693 - acc: 0.9700 - val_loss: 0.6063 - val_precision: 0.8053 - val_recall: 0.7969 - val_f1: 0.8011 - val_acc: 0.8021\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.29386\n",
            "Epoch 90/100\n",
            " - 43s - loss: 0.1305 - precision: 0.9565 - recall: 0.9519 - f1: 0.9542 - acc: 0.9567 - val_loss: 0.5274 - val_precision: 0.8900 - val_recall: 0.8854 - val_f1: 0.8877 - val_acc: 0.8854\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.29386\n",
            "Epoch 91/100\n",
            " - 44s - loss: 0.1483 - precision: 0.9515 - recall: 0.9423 - f1: 0.9468 - acc: 0.9459 - val_loss: 0.4419 - val_precision: 0.8931 - val_recall: 0.8750 - val_f1: 0.8837 - val_acc: 0.8802\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.29386\n",
            "Epoch 92/100\n",
            " - 43s - loss: 0.1517 - precision: 0.9445 - recall: 0.9411 - f1: 0.9428 - acc: 0.9435 - val_loss: 0.4985 - val_precision: 0.8144 - val_recall: 0.7552 - val_f1: 0.7836 - val_acc: 0.7760\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.29386\n",
            "Epoch 93/100\n",
            " - 43s - loss: 0.1361 - precision: 0.9551 - recall: 0.9459 - f1: 0.9504 - acc: 0.9531 - val_loss: 1.3918 - val_precision: 0.7344 - val_recall: 0.7344 - val_f1: 0.7344 - val_acc: 0.7344\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.29386\n",
            "Epoch 94/100\n",
            " - 41s - loss: 0.1808 - precision: 0.9269 - recall: 0.9171 - f1: 0.9219 - acc: 0.9195 - val_loss: 0.6490 - val_precision: 0.8226 - val_recall: 0.8021 - val_f1: 0.8121 - val_acc: 0.8229\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.29386\n",
            "Epoch 95/100\n",
            " - 40s - loss: 0.1620 - precision: 0.9452 - recall: 0.9315 - f1: 0.9383 - acc: 0.9387 - val_loss: 1.2210 - val_precision: 0.7672 - val_recall: 0.7552 - val_f1: 0.7611 - val_acc: 0.7552\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.29386\n",
            "Epoch 96/100\n",
            " - 41s - loss: 0.1756 - precision: 0.9414 - recall: 0.9255 - f1: 0.9333 - acc: 0.9339 - val_loss: 1.4473 - val_precision: 0.7054 - val_recall: 0.6979 - val_f1: 0.7016 - val_acc: 0.7031\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.29386\n",
            "Epoch 97/100\n",
            " - 40s - loss: 0.1888 - precision: 0.9505 - recall: 0.9267 - f1: 0.9384 - acc: 0.9423 - val_loss: 0.7173 - val_precision: 0.8298 - val_recall: 0.8125 - val_f1: 0.8211 - val_acc: 0.8177\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.29386\n",
            "Epoch 98/100\n",
            " - 39s - loss: 0.1563 - precision: 0.9540 - recall: 0.9471 - f1: 0.9505 - acc: 0.9531 - val_loss: 0.7019 - val_precision: 0.8206 - val_recall: 0.7865 - val_f1: 0.8031 - val_acc: 0.8021\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.29386\n",
            "Epoch 99/100\n",
            " - 40s - loss: 0.1519 - precision: 0.9452 - recall: 0.9351 - f1: 0.9401 - acc: 0.9399 - val_loss: 0.6397 - val_precision: 0.8166 - val_recall: 0.8125 - val_f1: 0.8145 - val_acc: 0.8177\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.29386\n",
            "Epoch 100/100\n",
            " - 39s - loss: 0.1358 - precision: 0.9587 - recall: 0.9471 - f1: 0.9528 - acc: 0.9543 - val_loss: 0.8484 - val_precision: 0.7474 - val_recall: 0.7396 - val_f1: 0.7434 - val_acc: 0.7396\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.29386\n",
            "Epoch 1/100\n",
            " - 55s - loss: 0.2983 - precision: 0.9202 - recall: 0.8858 - f1: 0.9026 - acc: 0.8990 - val_loss: 1.1406 - val_precision: 0.7333 - val_recall: 0.7292 - val_f1: 0.7312 - val_acc: 0.7292\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.14060, saving model to BREASTNET_FOLD_3.h5\n",
            "Epoch 2/100\n",
            " - 36s - loss: 0.3001 - precision: 0.9128 - recall: 0.8930 - f1: 0.9028 - acc: 0.9062 - val_loss: 0.6481 - val_precision: 0.8292 - val_recall: 0.8125 - val_f1: 0.8206 - val_acc: 0.8177\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.14060 to 0.64806, saving model to BREASTNET_FOLD_3.h5\n",
            "Epoch 3/100\n",
            " - 41s - loss: 0.2795 - precision: 0.9206 - recall: 0.9050 - f1: 0.9127 - acc: 0.9159 - val_loss: 0.8258 - val_precision: 0.6548 - val_recall: 0.6302 - val_f1: 0.6422 - val_acc: 0.6510\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.64806\n",
            "Epoch 4/100\n",
            " - 41s - loss: 0.2979 - precision: 0.9055 - recall: 0.8846 - f1: 0.8947 - acc: 0.8882 - val_loss: 0.6126 - val_precision: 0.7941 - val_recall: 0.7448 - val_f1: 0.7686 - val_acc: 0.7552\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.64806 to 0.61257, saving model to BREASTNET_FOLD_3.h5\n",
            "Epoch 5/100\n",
            " - 41s - loss: 0.1922 - precision: 0.9525 - recall: 0.9399 - f1: 0.9461 - acc: 0.9447 - val_loss: 0.2952 - val_precision: 0.8969 - val_recall: 0.8594 - val_f1: 0.8777 - val_acc: 0.8854\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.61257 to 0.29523, saving model to BREASTNET_FOLD_3.h5\n",
            "Epoch 6/100\n",
            " - 40s - loss: 0.1957 - precision: 0.9427 - recall: 0.9291 - f1: 0.9358 - acc: 0.9339 - val_loss: 0.4076 - val_precision: 0.8979 - val_recall: 0.8698 - val_f1: 0.8835 - val_acc: 0.8698\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.29523\n",
            "Epoch 7/100\n",
            " - 40s - loss: 0.2084 - precision: 0.9339 - recall: 0.9135 - f1: 0.9235 - acc: 0.9231 - val_loss: 0.4535 - val_precision: 0.8519 - val_recall: 0.8385 - val_f1: 0.8451 - val_acc: 0.8385\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.29523\n",
            "Epoch 8/100\n",
            " - 40s - loss: 0.2182 - precision: 0.9320 - recall: 0.9183 - f1: 0.9250 - acc: 0.9291 - val_loss: 0.3380 - val_precision: 0.8844 - val_recall: 0.8750 - val_f1: 0.8797 - val_acc: 0.8750\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.29523\n",
            "Epoch 9/100\n",
            " - 40s - loss: 0.2256 - precision: 0.9305 - recall: 0.9159 - f1: 0.9231 - acc: 0.9207 - val_loss: 0.5817 - val_precision: 0.8051 - val_recall: 0.7760 - val_f1: 0.7901 - val_acc: 0.7760\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.29523\n",
            "Epoch 10/100\n",
            " - 40s - loss: 0.2247 - precision: 0.9212 - recall: 0.8978 - f1: 0.9093 - acc: 0.9111 - val_loss: 0.2816 - val_precision: 0.8692 - val_recall: 0.8646 - val_f1: 0.8669 - val_acc: 0.8646\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.29523 to 0.28159, saving model to BREASTNET_FOLD_3.h5\n",
            "Epoch 11/100\n",
            " - 40s - loss: 0.2170 - precision: 0.9230 - recall: 0.9111 - f1: 0.9169 - acc: 0.9195 - val_loss: 0.3565 - val_precision: 0.9100 - val_recall: 0.8958 - val_f1: 0.9028 - val_acc: 0.9010\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.28159\n",
            "Epoch 12/100\n",
            " - 41s - loss: 0.2183 - precision: 0.9268 - recall: 0.9111 - f1: 0.9188 - acc: 0.9159 - val_loss: 0.5127 - val_precision: 0.8369 - val_recall: 0.8021 - val_f1: 0.8191 - val_acc: 0.8073\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.28159\n",
            "Epoch 13/100\n",
            " - 42s - loss: 0.1643 - precision: 0.9512 - recall: 0.9375 - f1: 0.9443 - acc: 0.9483 - val_loss: 0.3385 - val_precision: 0.8661 - val_recall: 0.8490 - val_f1: 0.8574 - val_acc: 0.8646\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.28159\n",
            "Epoch 14/100\n",
            " - 41s - loss: 0.1605 - precision: 0.9444 - recall: 0.9387 - f1: 0.9415 - acc: 0.9423 - val_loss: 0.1611 - val_precision: 0.9575 - val_recall: 0.9427 - val_f1: 0.9500 - val_acc: 0.9427\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.28159 to 0.16111, saving model to BREASTNET_FOLD_3.h5\n",
            "Epoch 15/100\n",
            " - 41s - loss: 0.1693 - precision: 0.9444 - recall: 0.9387 - f1: 0.9415 - acc: 0.9423 - val_loss: 0.5970 - val_precision: 0.8021 - val_recall: 0.7812 - val_f1: 0.7915 - val_acc: 0.7917\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.16111\n",
            "Epoch 16/100\n",
            " - 41s - loss: 0.1532 - precision: 0.9503 - recall: 0.9411 - f1: 0.9456 - acc: 0.9471 - val_loss: 0.3070 - val_precision: 0.9307 - val_recall: 0.8698 - val_f1: 0.8985 - val_acc: 0.8906\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.16111\n",
            "Epoch 17/100\n",
            " - 41s - loss: 0.1795 - precision: 0.9476 - recall: 0.9327 - f1: 0.9400 - acc: 0.9423 - val_loss: 0.2077 - val_precision: 0.9315 - val_recall: 0.9219 - val_f1: 0.9267 - val_acc: 0.9323\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.16111\n",
            "Epoch 18/100\n",
            " - 42s - loss: 0.2067 - precision: 0.9255 - recall: 0.9087 - f1: 0.9169 - acc: 0.9159 - val_loss: 0.3772 - val_precision: 0.8325 - val_recall: 0.8281 - val_f1: 0.8303 - val_acc: 0.8333\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.16111\n",
            "Epoch 19/100\n",
            " - 41s - loss: 0.1671 - precision: 0.9522 - recall: 0.9315 - f1: 0.9417 - acc: 0.9423 - val_loss: 0.2742 - val_precision: 0.8839 - val_recall: 0.8646 - val_f1: 0.8740 - val_acc: 0.8854\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.16111\n",
            "Epoch 20/100\n",
            " - 41s - loss: 0.1540 - precision: 0.9514 - recall: 0.9435 - f1: 0.9474 - acc: 0.9459 - val_loss: 0.2331 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.16111\n",
            "Epoch 21/100\n",
            " - 41s - loss: 0.1297 - precision: 0.9625 - recall: 0.9543 - f1: 0.9583 - acc: 0.9567 - val_loss: 0.2023 - val_precision: 0.9056 - val_recall: 0.9010 - val_f1: 0.9033 - val_acc: 0.9010\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.16111\n",
            "Epoch 22/100\n",
            " - 42s - loss: 0.1785 - precision: 0.9383 - recall: 0.9315 - f1: 0.9349 - acc: 0.9339 - val_loss: 0.3940 - val_precision: 0.8580 - val_recall: 0.8490 - val_f1: 0.8534 - val_acc: 0.8490\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.16111\n",
            "Epoch 23/100\n",
            " - 41s - loss: 0.1717 - precision: 0.9413 - recall: 0.9243 - f1: 0.9326 - acc: 0.9303 - val_loss: 0.3961 - val_precision: 0.8687 - val_recall: 0.8281 - val_f1: 0.8479 - val_acc: 0.8542\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.16111\n",
            "Epoch 24/100\n",
            " - 41s - loss: 0.1836 - precision: 0.9284 - recall: 0.9207 - f1: 0.9245 - acc: 0.9219 - val_loss: 0.6121 - val_precision: 0.7972 - val_recall: 0.7448 - val_f1: 0.7698 - val_acc: 0.7760\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.16111\n",
            "Epoch 25/100\n",
            " - 40s - loss: 0.1364 - precision: 0.9515 - recall: 0.9423 - f1: 0.9468 - acc: 0.9471 - val_loss: 0.3030 - val_precision: 0.9146 - val_recall: 0.8958 - val_f1: 0.9051 - val_acc: 0.9010\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.16111\n",
            "Epoch 26/100\n",
            " - 41s - loss: 0.1783 - precision: 0.9392 - recall: 0.9279 - f1: 0.9335 - acc: 0.9327 - val_loss: 0.2486 - val_precision: 0.9511 - val_recall: 0.9167 - val_f1: 0.9334 - val_acc: 0.9167\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.16111\n",
            "Epoch 27/100\n",
            " - 41s - loss: 0.1322 - precision: 0.9612 - recall: 0.9531 - f1: 0.9571 - acc: 0.9567 - val_loss: 0.1699 - val_precision: 0.9316 - val_recall: 0.9219 - val_f1: 0.9267 - val_acc: 0.9323\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.16111\n",
            "Epoch 28/100\n",
            " - 41s - loss: 0.1206 - precision: 0.9635 - recall: 0.9495 - f1: 0.9564 - acc: 0.9555 - val_loss: 0.4172 - val_precision: 0.8866 - val_recall: 0.8542 - val_f1: 0.8699 - val_acc: 0.8906\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.16111\n",
            "Epoch 29/100\n",
            " - 40s - loss: 0.2260 - precision: 0.9368 - recall: 0.9267 - f1: 0.9317 - acc: 0.9327 - val_loss: 2.1540 - val_precision: 0.5324 - val_recall: 0.4740 - val_f1: 0.5014 - val_acc: 0.5156\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.16111\n",
            "Epoch 30/100\n",
            " - 41s - loss: 0.2336 - precision: 0.9245 - recall: 0.9135 - f1: 0.9189 - acc: 0.9171 - val_loss: 1.0520 - val_precision: 0.8438 - val_recall: 0.8438 - val_f1: 0.8437 - val_acc: 0.8438\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.16111\n",
            "Epoch 31/100\n",
            " - 41s - loss: 0.1847 - precision: 0.9334 - recall: 0.9279 - f1: 0.9306 - acc: 0.9303 - val_loss: 0.8077 - val_precision: 0.7890 - val_recall: 0.7760 - val_f1: 0.7824 - val_acc: 0.7760\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.16111\n",
            "Epoch 32/100\n",
            " - 40s - loss: 0.2303 - precision: 0.9103 - recall: 0.9002 - f1: 0.9052 - acc: 0.9075 - val_loss: 0.3444 - val_precision: 0.8847 - val_recall: 0.8438 - val_f1: 0.8636 - val_acc: 0.8438\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.16111\n",
            "Epoch 33/100\n",
            " - 41s - loss: 0.2016 - precision: 0.9275 - recall: 0.9219 - f1: 0.9247 - acc: 0.9255 - val_loss: 0.2135 - val_precision: 0.9366 - val_recall: 0.9219 - val_f1: 0.9291 - val_acc: 0.9323\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.16111\n",
            "Epoch 34/100\n",
            " - 40s - loss: 0.1516 - precision: 0.9511 - recall: 0.9339 - f1: 0.9424 - acc: 0.9423 - val_loss: 0.2297 - val_precision: 0.9271 - val_recall: 0.9271 - val_f1: 0.9271 - val_acc: 0.9271\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.16111\n",
            "Epoch 35/100\n",
            " - 40s - loss: 0.1085 - precision: 0.9745 - recall: 0.9663 - f1: 0.9704 - acc: 0.9712 - val_loss: 0.3426 - val_precision: 0.9394 - val_recall: 0.8854 - val_f1: 0.9115 - val_acc: 0.8854\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.16111\n",
            "Epoch 36/100\n",
            " - 41s - loss: 0.1208 - precision: 0.9633 - recall: 0.9543 - f1: 0.9588 - acc: 0.9603 - val_loss: 0.1785 - val_precision: 0.9362 - val_recall: 0.9219 - val_f1: 0.9290 - val_acc: 0.9271\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.16111\n",
            "Epoch 37/100\n",
            " - 40s - loss: 0.1539 - precision: 0.9453 - recall: 0.9375 - f1: 0.9414 - acc: 0.9423 - val_loss: 0.1804 - val_precision: 0.9528 - val_recall: 0.9427 - val_f1: 0.9477 - val_acc: 0.9479\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.16111\n",
            "Epoch 38/100\n",
            " - 41s - loss: 0.1423 - precision: 0.9584 - recall: 0.9435 - f1: 0.9508 - acc: 0.9495 - val_loss: 0.1778 - val_precision: 0.9725 - val_recall: 0.9062 - val_f1: 0.9379 - val_acc: 0.9583\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.16111\n",
            "Epoch 39/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-12616612a704>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                         callbacks=callbacks)\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m                     \u001b[0;34m\"`use_multiprocessing=False, workers > 1`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m                     \"For more information see issue #1638.\")\n\u001b[0;32m--> 733\u001b[0;31m             \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                     \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m                     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwrap_exception\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_helper_reraises_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mnext_sample\u001b[0;34m(uid)\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0mnext\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mof\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0muid\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m     \"\"\"\n\u001b[0;32m--> 641\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_SHARED_SEQUENCES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-a13c82e099a9>\u001b[0m in \u001b[0;36mdata_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0;31m# x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m                 \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugmenting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_and_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-a13c82e099a9>\u001b[0m in \u001b[0;36mget_img\u001b[0;34m(self, img_path)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2765\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2766\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2767\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/kulak_colab/augmented_4/data/train/normal/normal (411).png'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRi9TjRd6vhF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_test_data():\n",
        "    gen = DATASET(SHAPE, BATCH_SIZE, range(1), BASE_DIR_TRAIN, BASE_DIR_TEST, SEED, TRAIN_TEST_RATIO, TOTAL_CLASS_NUMBER, augment=False).split_train_test(\"test\")\n",
        "                       \n",
        "    x = np.empty((len(gen[0]),)+SHAPE, dtype=np.float32)\n",
        "    y = np.empty((len(gen[1]), 4), dtype=np.float32)\n",
        "    \n",
        "    for ix, path in tqdm(enumerate(gen[0])):\n",
        "        img = np.array(Image.open(gen[0][ix]))\n",
        "        img = resize(img, SHAPE)\n",
        "\n",
        "        label = gen[1][ix]\n",
        "\n",
        "        x[ix] = img\n",
        "        y[ix] = label\n",
        "        \n",
        "    return x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geMqtF_O6vhM",
        "colab_type": "code",
        "outputId": "c92d744c-05b9-411e-f8bd-6e6627b93a53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x, y = get_test_data()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "172it [01:02,  3.18it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NJqVw766vhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Threshold predictions with THRESH_VAL\n",
        "def threshold_arr(array):\n",
        "    # Get all value from array\n",
        "    # Compare calue with THRESH_VAL \n",
        "    # IF value >= THRESH_VAL. round to 1\n",
        "    # ELSE. round to 0\n",
        "    new_arr = []\n",
        "    for ix, val in enumerate(array):\n",
        "        loc = np.array(val).argmax(axis=0)\n",
        "        k = list(np.zeros((len(val)), dtype=np.float32))\n",
        "        k[loc]=1\n",
        "        new_arr.append(k)\n",
        "        \n",
        "    return np.array(new_arr, dtype=np.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "if6Zitbe6vhT",
        "colab_type": "code",
        "outputId": "c377136c-db3f-4644-cd9e-697f3a4d57cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "models = []\n",
        "for i in range(3):\n",
        "    model = load_model(\"BREASTNET_FOLD_{}.h5\".format(i), custom_objects={'f1': f1, 'precision': precision, 'recall': recall})\n",
        "    print(model.evaluate(x, y, verbose=0))\n",
        "    models.append(model)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.2666433190190515, 0.9513061767400697, 0.8953488385954569, 0.9219531292139098, 0.901162792083829]\n",
            "[0.2162624067345331, 0.9467366698176362, 0.9186046525489452, 0.93235206326773, 0.9360465130140615]\n",
            "[0.08811203009167383, 0.9883720930232558, 0.9825581395348837, 0.9854189517886139, 0.9825581395348837]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXd8szMD6vhW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W87s-CRv6vha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_confusion_matrix(cm,\n",
        "                          target_names,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=None,\n",
        "                          normalize=True):\n",
        "    \"\"\"\n",
        "    given a sklearn confusion matrix (cm), make a nice plot\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
        "\n",
        "    target_names: given classification classes such as [0, 1, 2]\n",
        "                  the class names, for example: ['high', 'medium', 'low']\n",
        "\n",
        "    title:        the text to display at the top of the matrix\n",
        "\n",
        "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
        "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
        "                  plt.get_cmap('jet') or plt.cm.Blues\n",
        "\n",
        "    normalize:    If False, plot the raw numbers\n",
        "                  If True, plot the proportions\n",
        "\n",
        "    Usage\n",
        "    -----\n",
        "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
        "                                                              # sklearn.metrics.confusion_matrix\n",
        "                          normalize    = True,                # show proportions\n",
        "                          target_names = y_labels_vals,       # list of names of the classes\n",
        "                          title        = best_estimator_name) # title of graph\n",
        "\n",
        "    Citiation\n",
        "    ---------\n",
        "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
        "\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "    import itertools\n",
        "\n",
        "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
        "    misclass = 1 - accuracy\n",
        "\n",
        "    if cmap is None:\n",
        "        cmap = plt.get_cmap('Blues')\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "\n",
        "    if target_names is not None:\n",
        "        tick_marks = np.arange(len(target_names))\n",
        "        plt.xticks(tick_marks, target_names, rotation=45)\n",
        "        plt.yticks(tick_marks, target_names)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "\n",
        "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        if normalize:\n",
        "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "        else:\n",
        "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
        "    plt.savefig(\"confusion matrix_best.jpg\", dpi=150)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RL6FvbTk6vhh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 756
        },
        "outputId": "dd77c969-22c8-4fda-9fe1-341db2819f7a"
      },
      "source": [
        "y_preds = threshold_arr(models[2].predict(x, verbose=0))\n",
        "\n",
        "results = precision_recall_fscore_support(y, y_preds ,average='macro')\n",
        "acc = accuracy_score(y, y_preds)\n",
        "\n",
        "print(\"Accuracy: {}, F1_Score: {}, Precision: {}, Recall: {}\".format(acc, results[2], results[0], results[1]))\n",
        "print(\"\\n\")\n",
        "print(classification_report(y, y_preds))\n",
        "print(\"\\n\")\n",
        "cnf_matrix = confusion_matrix(y.argmax(axis=1), y_preds.argmax(axis=1))\n",
        "\n",
        "plot_confusion_matrix(cm           = cnf_matrix, \n",
        "                      normalize    = False,\n",
        "                      target_names = ['normal', 'aom', \"csom\", \"earwax\"],\n",
        "                      title        = \"Confusion Matrix\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9825581395348837, F1_Score: 0.9689368433273351, Precision: 0.9643302180685358, Recall: 0.9768302180685358\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99       107\n",
            "           1       1.00      0.92      0.96        24\n",
            "           2       0.87      1.00      0.93        13\n",
            "           3       1.00      1.00      1.00        28\n",
            "\n",
            "   micro avg       0.98      0.98      0.98       172\n",
            "   macro avg       0.96      0.98      0.97       172\n",
            "weighted avg       0.98      0.98      0.98       172\n",
            " samples avg       0.98      0.98      0.98       172\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfEAAAHCCAYAAAD/3PB+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqNUXD1Z6vhk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download('/content/BREASTNET_FOLD_3.h5') "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}